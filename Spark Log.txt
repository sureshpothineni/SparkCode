DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
DEBUG Utilities - Hive Conf not found or Session not initiated, use thread based class loader instead
INFO  metastore - Trying to connect to metastore with URI thrift://lbdp003a.rnd.pncint.net:9083
DEBUG UserGroupInformation - PrivilegedAction as:PL24116@PNCBANK.COM (auth:KERBEROS) from:org.apache.hadoop.hive.thrift.HadoopThrift
AuthBridge$Client.createClientTransport(HadoopThriftAuthBridge.java:213)
DEBUG UserGroupInformation - PrivilegedAction as:PL24116@PNCBANK.COM (auth:KERBEROS) from:org.apache.hadoop.hive.thrift.client.TUGIA
ssumingTransport.open(TUGIAssumingTransport.java:49)
DEBUG TSaslTransport - opening transport org.apache.thrift.transport.TSaslClientTransport@72a597f1
DEBUG TSaslClientTransport - Sending mechanism name GSSAPI and initial response of length 2497
DEBUG TSaslTransport - CLIENT: Writing message with status START and payload length 6
DEBUG TSaslTransport - CLIENT: Writing message with status OK and payload length 2497
DEBUG TSaslTransport - CLIENT: Start message handled
DEBUG TSaslTransport - CLIENT: Received message with status OK and payload length 104
DEBUG TSaslTransport - CLIENT: Writing message with status OK and payload length 0
DEBUG TSaslTransport - CLIENT: Received message with status OK and payload length 50
DEBUG TSaslTransport - CLIENT: Writing message with status COMPLETE and payload length 50
DEBUG TSaslTransport - CLIENT: Main negotiation loop complete
DEBUG TSaslTransport - CLIENT: SASL Client receiving last message
DEBUG TSaslTransport - CLIENT: Received message with status COMPLETE and payload length 0
INFO  metastore - Opened a connection to metastore, current connections: 1
INFO  metastore - Connected to metastore.
DEBUG TSaslTransport - writing data length: 30
DEBUG TSaslTransport - CLIENT: reading data length: 454
INFO  Hive - Registering function lat_lon_distance com.pncbank.hiveudf.lat_lon_distance
INFO  Hive - Registering function busdays_add com.pncbank.hiveudf.busdays_add
INFO  Hive - Registering function first_busday com.pncbank.hiveudf.first_busday
INFO  Hive - Registering function last_busday com.pncbank.hiveudf.last_busday
DEBUG TSaslTransport - writing data length: 88
DEBUG TSaslTransport - CLIENT: reading data length: 158
DEBUG Hive - Closing current thread's connection to Hive Metastore.
DEBUG TSaslTransport - writing data length: 21
INFO  metastore - Closed a connection to metastore, current connections: 0
DEBUG Client - Attempting to fetch HBase security token.
DEBUG DFSClient - /user/pl24116/.sparkStaging/application_1488308813765_0068: masked={ masked: rwxr-xr-x, unmasked: rwxrwxrwx }
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #3
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #3
DEBUG ProtobufRpcEngine - Call: mkdirs took 16ms
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #4
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #4
DEBUG ProtobufRpcEngine - Call: setPermission took 754ms
INFO  Client - Uploading resource file:/tmp/spark-7a04ff43-c1c3-462f-b9b1-c67318b6ab43/__spark_conf__1833562380849170421.zip -> hdfs
://nameservice1/user/pl24116/.sparkStaging/application_1488308813765_0068/__spark_conf__1833562380849170421.zip
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #5
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #5
DEBUG ProtobufRpcEngine - Call: getFileInfo took 1ms
DEBUG DFSClient - /user/pl24116/.sparkStaging/application_1488308813765_0068/__spark_conf__1833562380849170421.zip: masked={ masked:
 rw-r--r--, unmasked: rw-rw-rw- }
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #6
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #6
DEBUG ProtobufRpcEngine - Call: create took 3ms
DEBUG DFSClient - computePacketChunkSize: src=/user/pl24116/.sparkStaging/application_1488308813765_0068/__spark_conf__1833562380849
170421.zip, chunkSize=516, chunksPerPacket=127, packetSize=65532
DEBUG LeaseRenewer - Lease renewer daemon for [DFSClient_NONMAPREDUCE_-279890539_1] with renew id 1 started
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=0, src=/user/pl24116/.sparkStaging/application_1488308813765_0068
/__spark_conf__1833562380849170421.zip, packetSize=65532, chunksPerPacket=127, bytesCurBlock=0
DEBUG DFSClient - Queued packet 0
DEBUG DFSClient - Queued packet 1
DEBUG DFSClient - Allocating new block
DEBUG DFSClient - Waiting for ack for: 1
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #7
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #7
DEBUG ProtobufRpcEngine - Call: addBlock took 4ms
DEBUG DFSClient - pipeline = DatanodeInfoWithStorage[10.3.113.29:1004,DS-b9d98861-f3b6-4ed7-b028-1f75de7adf19,DISK]
DEBUG DFSClient - pipeline = DatanodeInfoWithStorage[10.3.113.28:1004,DS-7b63f65a-ed96-4159-94da-7f483b1a2c3d,DISK]
DEBUG DFSClient - pipeline = DatanodeInfoWithStorage[10.3.113.22:1004,DS-2aa527f0-181d-48ed-b337-696ef2dc762f,DISK]
DEBUG DFSClient - Connecting to datanode 10.3.113.29:1004
DEBUG DFSClient - Send buf size 124928
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #8
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #8
DEBUG ProtobufRpcEngine - Call: getServerDefaults took 1ms
DEBUG SaslDataTransferClient - SASL client skipping handshake in secured configuration with privileged port for addr = /10.3.113.29,
 datanodeId = DatanodeInfoWithStorage[10.3.113.29:1004,DS-b9d98861-f3b6-4ed7-b028-1f75de7adf19,DISK]
DEBUG DFSClient - nodes [DatanodeInfoWithStorage[10.3.113.29:1004,DS-b9d98861-f3b6-4ed7-b028-1f75de7adf19,DISK], DatanodeInfoWithSto
rage[10.3.113.28:1004,DS-7b63f65a-ed96-4159-94da-7f483b1a2c3d,DISK], DatanodeInfoWithStorage[10.3.113.22:1004,DS-2aa527f0-181d-48ed-
b337-696ef2dc762f,DISK]] storageTypes [DISK, DISK, DISK] storageIDs [DS-b9d98861-f3b6-4ed7-b028-1f75de7adf19, DS-7b63f65a-ed96-4159-
94da-7f483b1a2c3d, DS-2aa527f0-181d-48ed-b337-696ef2dc762f]
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229041_8583451 sending packet packet seqno: 0 o
ffsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 36735
DEBUG DFSClient - DFSClient seqno: 0 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 2167111
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229041_8583451 sending packet packet seqno: 1 o
ffsetInBlock: 36735 lastPacketInBlock: true lastByteOffsetInBlock: 36735
DEBUG DFSClient - DFSClient seqno: 1 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 1662058
DEBUG DFSClient - Closing old block BP-1978894624-10.3.112.24-1435864746178:blk_1082229041_8583451
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #9
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #9
DEBUG ProtobufRpcEngine - Call: complete took 3ms
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #10
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #10
DEBUG ProtobufRpcEngine - Call: setReplication took 5ms
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #11
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #11
DEBUG ProtobufRpcEngine - Call: setPermission took 3ms
DEBUG UserGroupInformation - PrivilegedAction as:PL24116@PNCBANK.COM (auth:KERBEROS) from:org.apache.hadoop.fs.FileContext.getAbstra
ctFileSystem(FileContext.java:335)
DEBUG BlockReaderLocal - dfs.client.use.legacy.blockreader.local = false
DEBUG BlockReaderLocal - dfs.client.read.shortcircuit = false
DEBUG BlockReaderLocal - dfs.client.domain.socket.data.traffic = false
DEBUG BlockReaderLocal - dfs.domain.socket.path = /var/run/hdfs-sockets/dn
DEBUG HAUtil - No HA service delegation token found for logical URI hdfs://nameservice1/user/pl24116/.sparkStaging/application_14883
08813765_0068/__spark_conf__1833562380849170421.zip
DEBUG BlockReaderLocal - dfs.client.use.legacy.blockreader.local = false
DEBUG BlockReaderLocal - dfs.client.read.shortcircuit = false
DEBUG BlockReaderLocal - dfs.client.domain.socket.data.traffic = false
DEBUG BlockReaderLocal - dfs.domain.socket.path = /var/run/hdfs-sockets/dn
DEBUG RetryUtils - multipleLinearRandomRetry = null
DEBUG Client - getting client out of cache: org.apache.hadoop.ipc.Client@596df59
DEBUG DataTransferSaslUtil - DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.trans
fer.protection
DEBUG OpensslAesCtrCryptoCodec - Using org.apache.hadoop.crypto.random.OsSecureRandom as random number generator.
DEBUG PerformanceAdvisory - Using crypto codec org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec.
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #12
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #12
DEBUG ProtobufRpcEngine - Call: getFileInfo took 1ms
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #13
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #13
DEBUG ProtobufRpcEngine - Call: getFileInfo took 1042ms
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #14
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #14
DEBUG ProtobufRpcEngine - Call: getFileInfo took 1ms
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #15
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #15
DEBUG ProtobufRpcEngine - Call: getFileInfo took 1ms
DEBUG Client - ===============================================================================
DEBUG Client - YARN AM launch context:
DEBUG Client -     user class: N/A
DEBUG Client -     env:
DEBUG Client -         SPARK_YARN_CACHE_ARCHIVES -> hdfs://nameservice1/user/pl24116/.sparkStaging/application_1488308813765_0068/__
spark_conf__1833562380849170421.zip#__spark_conf__
DEBUG Client -         CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0
.41/lib/spark/lib/spark-assembly.jar<CPS>$HADOOP_CLIENT_CONF_DIR<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/*<CPS>$HADOOP_COMMON_H
OME/lib/*<CPS>$HADOOP_HDFS_HOME/*<CPS>$HADOOP_HDFS_HOME/lib/*<CPS>$HADOOP_YARN_HOME/*<CPS>$HADOOP_YARN_HOME/lib/*<CPS>/var/lib/sqoop
/*<CPS>$HADOOP_MAPRED_HOME/*<CPS>$HADOOP_MAPRED_HOME/lib/*<CPS>$MR2_CLASSPATH<CPS>{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.
10.0.p0.41/jars/ST4-4.0.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/accumulo-core-1.6.0.jar:{{HADOOP_COM
MON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/accumulo-fate-1.6.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/accumulo-start-1.6.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/accumulo-trace-1.6.0.jar:{{HA
DOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/activation-1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.1
0.0.p0.41/jars/ant-1.9.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ant-launcher-1.9.1.jar:{{HADOOP_COMMO
N_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/antlr-2.7.7.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jar
s/antlr-runtime-3.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aopalliance-1.0.jar:{{HADOOP_COMMON_HOME}}
/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/apache-log4j-extras-1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41
/jars/apache-log4j-extras-1.2.17.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/apacheds-i18n-2.0.0-M15.jar:{
{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/apacheds-kerberos-codec-2.0.0-M15.jar:{{HADOOP_COMMON_HOME}}/../../
../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/api-asn1-api-1.0.0-M20.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ap
i-util-1.0.0-M20.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/asm-3.2.jar:{{HADOOP_COMMON_HOME}}/../../../C
DH-5.10.0-1.cdh5.10.0.p0.41/jars/asm-commons-3.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/asm-tree-3.1.
jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/async-1.4.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.c
dh5.10.0.p0.41/jars/asynchbase-1.7.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-1.7.6-cdh5.10.0.jar:
{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-compiler-1.7.6-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../
CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-ipc-1.7.6-cdh5.10.0-tests.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/ja
rs/avro-ipc-1.7.6-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-mapred-1.7.6-cdh5.10.0-hadoop
2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-maven-plugin-1.7.6-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}
/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-protobuf-1.7.6-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/avro-service-archetype-1.7.6-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-thrift
-1.7.6-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aws-java-sdk-core-1.10.6.jar:{{HADOOP_COMMON_
HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aws-java-sdk-kms-1.10.6.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/aws-java-sdk-s3-1.10.6.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aws-java-sdk-sts-1.10.6.jar
:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/bonecp-0.8.0.RELEASE.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10
.0-1.cdh5.10.0.p0.41/jars/calcite-avatica-1.0.0-incubating.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/cal
cite-core-1.0.0-incubating.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/calcite-linq4j-1.0.0-incubating.jar
:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-beanutils-1.9.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5
.10.0-1.cdh5.10.0.p0.41/jars/commons-beanutils-core-1.8.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/comm
ons-cli-1.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-codec-1.4.jar:{{HADOOP_COMMON_HOME}}/../..
/../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-codec-1.8.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/common
s-collections-3.2.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-compiler-2.7.6.jar:{{HADOOP_COMMON
_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-compress-1.4.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/commons-configuration-1.6.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-daemon-1.0.13.ja
r:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-dbcp-1.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-
1.cdh5.10.0.p0.41/jars/commons-digester-1.8.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-el-1.0.jar
:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-httpclient-3.0.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/commons-httpclient-3.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-i
o-2.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-jexl-2.1.1.jar:{{HADOOP_COMMON_HOME}}/../../../C
DH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-lang-2.6.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-lang
3-3.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-logging-1.1.3.jar:{{HADOOP_COMMON_HOME}}/../../.
./CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-math-2.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-m
ath3-3.1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-net-3.1.jar:{{HADOOP_COMMON_HOME}}/../../..
/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-pool-1.5.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-
vfs2-2.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-client-2.6.0.jar:{{HADOOP_COMMON_HOME}}/../..
/../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-client-2.7.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/cur
ator-framework-2.6.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-framework-2.7.1.jar:{{HADOOP_COMM
ON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-recipes-2.6.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/curator-recipes-2.7.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/datanucleus-api-jdo-3.2.6.j
ar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/datanucleus-core-3.2.10.jar:{{HADOOP_COMMON_HOME}}/../../../CDH
-5.10.0-1.cdh5.10.0.p0.41/jars/datanucleus-rdbms-3.2.9.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/derby-1
0.11.1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/eigenbase-properties-1.1.4.jar:{{HADOOP_COMMON_HOME}}
/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/fastutil-6.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/findb
ugs-annotations-1.3.9-1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-avro-source-1.6.0-cdh5.10.0.jar:
{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-dataset-sink-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../.
./../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-file-channel-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/flume-hdfs-sink-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-hive-sink-1.
6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-irc-sink-1.6.0-cdh5.10.0.jar:{{HADOOP_COMM
ON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-jdbc-channel-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10
.0-1.cdh5.10.0.p0.41/jars/flume-jms-source-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flu
me-kafka-channel-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-kafka-source-1.6.0-cdh5
.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-auth-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}
/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-configuration-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.
cdh5.10.0.p0.41/jars/flume-ng-core-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-el
asticsearch-sink-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-embedded-agent-1.6.0
-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-hbase-sink-1.6.0-cdh5.10.0.jar:{{HADOOP_CO
MMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-kafka-sink-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5
.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-log4jappender-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41
/jars/flume-ng-morphline-solr-sink-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-no
de-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-sdk-1.6.0-cdh5.10.0.jar:{{HADOOP_C
OMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-scribe-source-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/flume-spillable-memory-channel-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.1
0.0.p0.41/jars/flume-taildir-source-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-thri
ft-source-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-tools-1.6.0-cdh5.10.0.jar:{{HA
DOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-twitter-source-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../
../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/geronimo-annotation_1.0_spec-1.1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p
0.41/jars/geronimo-jaspic_1.0_spec-1.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/geronimo-jta_1.1_spec-1
.1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/groovy-all-2.4.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/gson-2.2.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guava-11.0.2.jar:{{HA
DOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guava-11.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.
p0.41/jars/guava-14.0.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guice-3.0.jar:{{HADOOP_COMMON_HOME}}/.
./../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guice-servlet-3.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ha
doop-annotations-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-ant-2.6.0-cdh5.10.0.ja
r:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-archive-logs-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/.
./../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-archives-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/hadoop-auth-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-aws-2.6.0-cdh5.
10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-azure-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/.
./../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-common-2.6.0-cdh5.10.0-tests.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.
10.0.p0.41/jars/hadoop-common-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-datajoin-
2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-distcp-2.6.0-cdh5.10.0.jar:{{HADOOP_COM
MON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-extras-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1
.cdh5.10.0.p0.41/jars/hadoop-gridmix-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-hd
fs-2.6.0-cdh5.10.0-tests.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-hdfs-2.6.0-cdh5.10.0.jar:{{HAD
OOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-hdfs-nfs-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH
-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-app-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/hadoop-mapreduce-client-common-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ha
doop-mapreduce-client-core-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-cl
ient-hs-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-hs-plugins-2.6
.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.10.0
-tests.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.10.0.jar:{
{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-nativetask-2.6.0-cdh5.10.0.jar:{{HADOOP_COM
MON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../
../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-examples-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.c
dh5.10.0.p0.41/jars/hadoop-nfs-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-openstac
k-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-rumen-2.6.0-cdh5.10.0.jar:{{HADOOP_CO
MMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-sls-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.c
dh5.10.0.p0.41/jars/hadoop-streaming-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-ya
rn-api-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-applications-distributedshe
ll-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-applications-unmanaged-am-launc
her-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-client-2.6.0-cdh5.10.0.jar:{{H
ADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-common-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../.
./CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-registry-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.
p0.41/jars/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p
0.41/jars/hadoop-yarn-server-common-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yar
n-server-nodemanager-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-resour
cemanager-2.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-tests-2.6.0-cdh5.
10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-web-proxy-2.6.0-cdh5.10.0.jar:{{HADOOP
_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hamcrest-core-1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10
.0.p0.41/jars/hamcrest-core-1.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-annotations-1.2.0-cdh5.1
0.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-client-1.2.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/..
/../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-common-1.2.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.
41/jars/hbase-hadoop-compat-1.2.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-hadoop2-comp
at-1.2.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-protocol-1.2.0-cdh5.10.0.jar:{{HADOOP
_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-server-1.2.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.
0-1.cdh5.10.0.p0.41/jars/high-scale-lib-1.1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-accumulo-ha
ndler-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-ant-1.1.0-cdh5.10.0.jar:{{HADOOP_CO
MMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-beeline-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1
.cdh5.10.0.p0.41/jars/hive-cli-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1
.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-contrib-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_
HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-exec-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10
.0.p0.41/jars/hive-hbase-handler-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-hwi-1.1.
0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-jdbc-1.1.0-cdh5.10.0-standalone.jar:{{HADOOP_
COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-jdbc-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.
cdh5.10.0.p0.41/jars/hive-metastore-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-serde
-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-service-1.1.0-cdh5.10.0.jar:{{HADOOP_COM
MON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-shims-0.23-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0
-1.cdh5.10.0.p0.41/jars/hive-shims-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-shims-
common-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-shims-scheduler-1.1.0-cdh5.10.0.ja
r:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-testutils-1.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../
../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/htrace-core-3.2.0-incubating.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/j
ars/htrace-core4-4.0.1-incubating.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/httpclient-4.2.5.jar:{{HADOO
P_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/httpcore-4.2.5.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/hue-plugins-3.9.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/irclib-1.10.jar:{{HADO
OP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ivy-2.0.0-rc2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/jackson-annotations-2.2.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-core-2.2.3.jar:{
{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-core-asl-1.8.8.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10
.0-1.cdh5.10.0.p0.41/jars/jackson-databind-2.2.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-jaxrs
-1.8.8.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-mapper-asl-1.8.8.jar:{{HADOOP_COMMON_HOME}}/../
../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-xc-1.8.8.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jamon
-runtime-2.3.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/janino-2.7.6.jar:{{HADOOP_COMMON_HOME}}/../../.
./CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jasper-compiler-5.5.23.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jas
per-runtime-5.5.23.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/java-xmlbuilder-0.4.jar:{{HADOOP_COMMON_HOM
E}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/javax.inject-1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/
jaxb-api-2.2.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jaxb-impl-2.2.3-1.jar:{{HADOOP_COMMON_HOME}}/..
/../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jcommander-1.32.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jdo-a
pi-3.0.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-client-1.9.jar:{{HADOOP_COMMON_HOME}}/../../..
/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-core-1.9.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-guic
e-1.9.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-json-1.9.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/jersey-server-1.9.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jets3t-0.9.0.j
ar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jettison-1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.c
dh5.10.0.p0.41/jars/jetty-6.1.26.cloudera.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jetty-all-7.6.0.v2
0120127.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jetty-all-server-7.6.0.v20120127.jar:{{HADOOP_COMMON_H
OME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jetty-util-6.1.26.cloudera.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.
10.0.p0.41/jars/jline-2.11.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jline-2.12.jar:{{HADOOP_COMMON_HOME
}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/joda-time-1.6.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jo
da-time-2.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jopt-simple-4.9.jar:{{HADOOP_COMMON_HOME}}/../../.
./CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jpam-1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jsch-0.1.42.jar:{
{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jsp-api-2.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.1
0.0.p0.41/jars/jsr305-1.3.9.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jsr305-3.0.0.jar:{{HADOOP_COMMON_H
OME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jta-1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/junit
-4.11.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kafka-clients-0.9.0-kafka-2.0.2.jar:{{HADOOP_COMMON_HOME
}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kafka_2.10-0.9.0-kafka-2.0.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/kite-data-core-1.0.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kite-data-hbase-1.
0.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kite-data-hive-1.0.0-cdh5.10.0.jar:{{HADOOP_COMM
ON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kite-hadoop-compatibility-1.0.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../C
DH-5.10.0-1.cdh5.10.0.p0.41/jars/leveldbjni-all-1.8.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/libfb303-0
.9.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/libthrift-0.9.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5
.10.0-1.cdh5.10.0.p0.41/jars/log4j-1.2.16.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/log4j-1.2.17.jar:{{H
ADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/logredactor-1.0.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cd
h5.10.0.p0.41/jars/lz4-1.3.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/mail-1.4.1.jar:{{HADOOP_COMMON_HO
ME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/mapdb-0.9.9.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ma
ven-scm-api-1.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/maven-scm-provider-svn-commons-1.4.jar:{{HADOO
P_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/maven-scm-provider-svnexe-1.4.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.1
0.0-1.cdh5.10.0.p0.41/jars/metrics-core-2.2.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/metrics-core-3.0
.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/metrics-json-3.0.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/metrics-jvm-3.0.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/microsoft-wind
owsazure-storage-sdk-0.6.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/mina-core-2.0.4.jar:{{HADOOP_COMMON
_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/mockito-all-1.8.5.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.4
1/jars/netty-3.10.5.Final.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/netty-3.9.4.Final.jar:{{HADOOP_COMMO
N_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/netty-all-4.0.23.Final.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/opencsv-2.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/oro-2.0.8.jar:{{HADOOP_COMMON_HOME}}/
../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/paranamer-2.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parqu
et-avro-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-cascading-1.5.0-cdh5.10.0.jar:
{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-column-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../..
/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-common-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/j
ars/parquet-encoding-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-format-2.1.0-cdh5
.10.0-javadoc.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-format-2.1.0-cdh5.10.0-sources.jar:{{HAD
OOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-format-2.1.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/parquet-generator-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jar
s/parquet-hadoop-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-hadoop-bundle-1.5.0-c
dh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-jackson-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_H
OME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-pig-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.1
0.0.p0.41/jars/parquet-pig-bundle-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-prot
obuf-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-scala_2.10-1.5.0-cdh5.10.0.jar:{{
HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-scrooge_2.10-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../.
./../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-test-hadoop2-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10
.0.p0.41/jars/parquet-thrift-1.5.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-tools-1.5
.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:{{H
ADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/plexus-utils-1.5.6.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.c
dh5.10.0.p0.41/jars/protobuf-java-2.5.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/regexp-1.3.jar:{{HADOO
P_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/scala-library-2.10.5.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh
5.10.0.p0.41/jars/serializer-2.7.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/servlet-api-2.5-20110124.ja
r:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/servlet-api-2.5.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1
.cdh5.10.0.p0.41/jars/slf4j-api-1.7.5.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/slf4j-log4j12-1.7.5.jar:
{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/snappy-java-1.0.4.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0
-1.cdh5.10.0.p0.41/jars/spark-1.6.0-cdh5.10.0-yarn-shuffle.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/spa
rk-streaming-flume-sink_2.10-1.6.0-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/stax-api-1.0-2.ja
r:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/stax-api-1.0.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.
cdh5.10.0.p0.41/jars/stringtemplate-3.2.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/super-csv-2.2.0.jar:
{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/tempus-fugit-1.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.
cdh5.10.0.p0.41/jars/trevni-avro-1.7.6-cdh5.10.0-hadoop2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/trevn
i-avro-1.7.6-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/trevni-core-1.7.6-cdh5.10.0.jar:{{HADOO
P_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/twitter4j-core-3.0.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh
5.10.0.p0.41/jars/twitter4j-media-support-3.0.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/twitter4j-stre
am-3.0.3.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/unused-1.0.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/velocity-1.5.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/velocity-1.7.jar:{{
HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/xalan-2.7.2.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10
.0.p0.41/jars/xercesImpl-2.9.1.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/xml-apis-1.3.04.jar:{{HADOOP_CO
MMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/xmlenc-0.52.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/
jars/xz-1.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/jars/zkclient-0.7.jar:{{HADOOP_COMMON_HOME}}/../../../C
DH-5.10.0-1.cdh5.10.0.p0.41/jars/zookeeper-3.4.5-cdh5.10.0.jar:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hado
op/LICENSE.txt:{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/NOTICE.txt
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM: closed
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM: stopped, rema
ining connections 1
DEBUG Client -         SPARK_DIST_CLASSPATH -> /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ST4-4.0.4.jar:/opt/cloudera/p
arcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/accumulo-core-1.6.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/accumulo-fa
te-1.6.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/accumulo-start-1.6.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh
5.10.0.p0.41/jars/accumulo-trace-1.6.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/activation-1.1.jar:/opt/cloudera/
parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ant-1.9.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ant-launcher-1.9.1.j
ar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/antlr-2.7.7.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/a
ntlr-runtime-3.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aopalliance-1.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.
cdh5.10.0.p0.41/jars/apache-log4j-extras-1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/apache-log4j-extras-1.2.17.
jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/apacheds-i18n-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/api-asn1-api-1.0.0-M20.jar
:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/api-util-1.0.0-M20.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/j
ars/asm-3.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/asm-commons-3.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.
10.0.p0.41/jars/asm-tree-3.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/async-1.4.0.jar:/opt/cloudera/parcels/CDH-5
.10.0-1.cdh5.10.0.p0.41/jars/asynchbase-1.7.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-1.7.6-cdh5.10.0.jar:/
opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-compiler-1.7.6-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/avro-ipc-1.7.6-cdh5.10.0-tests.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-ipc-1.7.6-cdh5.10.0.jar
:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-mapred-1.7.6-cdh5.10.0-hadoop2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.
cdh5.10.0.p0.41/jars/avro-maven-plugin-1.7.6-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-protobuf-1.7
.6-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-service-archetype-1.7.6-cdh5.10.0.jar:/opt/cloudera/pa
rcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/avro-thrift-1.7.6-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aws-
java-sdk-core-1.10.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aws-java-sdk-kms-1.10.6.jar:/opt/cloudera/parcels/C
DH-5.10.0-1.cdh5.10.0.p0.41/jars/aws-java-sdk-s3-1.10.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/aws-java-sdk-sts
-1.10.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/bonecp-0.8.0.RELEASE.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5
.10.0.p0.41/jars/calcite-avatica-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/calcite-core-1.0.0-inc
ubating.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/calcite-linq4j-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.
10.0-1.cdh5.10.0.p0.41/jars/commons-beanutils-1.9.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-beanutils-co
re-1.8.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-cli-1.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/commons-codec-1.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-codec-1.8.jar:/opt/cloudera/parce
ls/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-collections-3.2.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-c
ompiler-2.7.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-compress-1.4.1.jar:/opt/cloudera/parcels/CDH-5.10.
0-1.cdh5.10.0.p0.41/jars/commons-configuration-1.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-daemon-1.0.13
.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-dbcp-1.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41
/jars/commons-digester-1.8.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-el-1.0.jar:/opt/cloudera/parcels/CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/commons-httpclient-3.0.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-httpclien
t-3.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-io-2.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0
.41/jars/commons-jexl-2.1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-lang-2.6.jar:/opt/cloudera/parcels/C
DH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-lang3-3.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-logging-1.1.3
.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-math-2.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41
/jars/commons-math3-3.1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-net-3.1.jar:/opt/cloudera/parcels/CDH-
5.10.0-1.cdh5.10.0.p0.41/jars/commons-pool-1.5.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/commons-vfs2-2.0.jar:/o
pt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-client-2.6.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/ja
rs/curator-client-2.7.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-framework-2.6.0.jar:/opt/cloudera/parcel
s/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-framework-2.7.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-reci
pes-2.6.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/curator-recipes-2.7.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.c
dh5.10.0.p0.41/jars/datanucleus-api-jdo-3.2.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/datanucleus-core-3.2.10.ja
r:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/datanucleus-rdbms-3.2.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p
0.41/jars/derby-10.11.1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/eigenbase-properties-1.1.4.jar:/opt/cloudera/p
arcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/fastutil-6.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/findbugs-annotatio
ns-1.3.9-1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-avro-source-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/C
DH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-dataset-sink-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flum
e-file-channel-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-hdfs-sink-1.6.0-cdh5.10.0.jar:/opt/
cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-hive-sink-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.
p0.41/jars/flume-irc-sink-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-jdbc-channel-1.6.0-cdh5.
10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-jms-source-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10
.0-1.cdh5.10.0.p0.41/jars/flume-kafka-channel-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-kafk
a-source-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-auth-1.6.0-cdh5.10.0.jar:/opt/cloudera
/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-configuration-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p
0.41/jars/flume-ng-core-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-elasticsearch-sink-1.6.
0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-embedded-agent-1.6.0-cdh5.10.0.jar:/opt/cloudera/pa
rcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-hbase-sink-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/j
ars/flume-ng-kafka-sink-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-log4jappender-1.6.0-cdh
5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-morphline-solr-sink-1.6.0-cdh5.10.0.jar:/opt/cloudera/pa
rcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-ng-node-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/fl
ume-ng-sdk-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-scribe-source-1.6.0-cdh5.10.0.jar:/opt/
cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-spillable-memory-channel-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10
.0-1.cdh5.10.0.p0.41/jars/flume-taildir-source-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-thr
ift-source-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-tools-1.6.0-cdh5.10.0.jar:/opt/cloudera
/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/flume-twitter-source-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.
41/jars/geronimo-annotation_1.0_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/geronimo-jaspic_1.0_spec-1.0.
jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/geronimo-jta_1.1_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.
10.0.p0.41/jars/groovy-all-2.4.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/gson-2.2.4.jar:/opt/cloudera/parcels/CD
H-5.10.0-1.cdh5.10.0.p0.41/jars/guava-11.0.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guava-11.0.jar:/opt/clouder
a/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guava-14.0.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guice-3.0.jar:/
opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/guice-servlet-3.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars
/hadoop-annotations-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-ant-2.6.0-cdh5.10.0.jar:/opt/
cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-archive-logs-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.1
0.0.p0.41/jars/hadoop-archives-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-auth-2.6.0-cdh5.10
.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-aws-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh
5.10.0.p0.41/jars/hadoop-azure-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-common-2.6.0-cdh5.
10.0-tests.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-common-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5
.10.0-1.cdh5.10.0.p0.41/jars/hadoop-datajoin-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-dist
cp-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-extras-2.6.0-cdh5.10.0.jar:/opt/cloudera/parce
ls/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-gridmix-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hado
op-hdfs-2.6.0-cdh5.10.0-tests.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-hdfs-2.6.0-cdh5.10.0.jar:/opt/cloud
era/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-hdfs-nfs-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41
/jars/hadoop-mapreduce-client-app-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-clien
t-common-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-core-2.6.0-cdh5.10.0.ja
r:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-hs-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5
.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.
41/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.10.0-tests.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-m
apreduce-client-jobclient-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-native
task-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.10.0.jar
:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-mapreduce-examples-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.1
0.0-1.cdh5.10.0.p0.41/jars/hadoop-nfs-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-openstack-2
.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-rumen-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CD
H-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-sls-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-stream
ing-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-api-2.6.0-cdh5.10.0.jar:/opt/cloudera/pa
rcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-applications-distributedshell-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10
.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5
.10.0.p0.41/jars/hadoop-yarn-client-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-common-2
.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-registry-2.6.0-cdh5.10.0.jar:/opt/cloudera/pa
rcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5
.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-common-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/h
adoop-yarn-server-nodemanager-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-resourc
emanager-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-tests-2.6.0-cdh5.10.0.jar:/o
pt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hadoop-yarn-server-web-proxy-2.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.1
0.0-1.cdh5.10.0.p0.41/jars/hamcrest-core-1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hamcrest-core-1.3.jar:/opt/
cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-annotations-1.2.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/hbase-client-1.2.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-common-1.2.0-cdh5.10.0.j
ar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-hadoop-compat-1.2.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-
1.cdh5.10.0.p0.41/jars/hbase-hadoop2-compat-1.2.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-protoc
ol-1.2.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hbase-server-1.2.0-cdh5.10.0.jar:/opt/cloudera/parcel
s/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/high-scale-lib-1.1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-accumulo-h
andler-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-ant-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcel
s/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-beeline-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-cl
i-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-common-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/
CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-contrib-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-exec
-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-hbase-handler-1.1.0-cdh5.10.0.jar:/opt/cloudera/pa
rcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-hwi-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-jd
bc-1.1.0-cdh5.10.0-standalone.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-jdbc-1.1.0-cdh5.10.0.jar:/opt/clouder
a/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-metastore-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/ja
rs/hive-serde-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-service-1.1.0-cdh5.10.0.jar:/opt/clou
dera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-shims-0.23-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.4
1/jars/hive-shims-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-shims-common-1.1.0-cdh5.10.0.jar:
/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hive-shims-scheduler-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.
cdh5.10.0.p0.41/jars/hive-testutils-1.1.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/htrace-core-3.2.0-in
cubating.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/htrace-core4-4.0.1-incubating.jar:/opt/cloudera/parcels/CDH-5.1
0.0-1.cdh5.10.0.p0.41/jars/httpclient-4.2.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/httpcore-4.2.5.jar:/opt/clou
dera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/hue-plugins-3.9.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/ja
rs/irclib-1.10.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/ivy-2.0.0-rc2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5
.10.0.p0.41/jars/jackson-annotations-2.2.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-core-2.2.3.jar:/opt/c
loudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-core-asl-1.8.8.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars
/jackson-databind-2.2.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-jaxrs-1.8.8.jar:/opt/cloudera/parcels/CD
H-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-mapper-asl-1.8.8.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jackson-xc-1.8.
8.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jamon-runtime-2.3.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p
0.41/jars/janino-2.7.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jasper-compiler-5.5.23.jar:/opt/cloudera/parcels/
CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jasper-runtime-5.5.23.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/java-xmlbuilder-
0.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/javax.inject-1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.4
1/jars/jaxb-api-2.2.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jaxb-impl-2.2.3-1.jar:/opt/cloudera/parcels/CDH-5.
10.0-1.cdh5.10.0.p0.41/jars/jcommander-1.32.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jdo-api-3.0.1.jar:/opt/cloud
era/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-client-1.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-c
ore-1.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-guice-1.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/jersey-json-1.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jersey-server-1.9.jar:/opt/cloudera/parcels
/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jets3t-0.9.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jettison-1.1.jar:/opt/cl
oudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jetty-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars
/jetty-all-7.6.0.v20120127.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jetty-all-server-7.6.0.v20120127.jar:/opt/clo
udera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jetty-util-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/
jars/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jline-2.12.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.1
0.0.p0.41/jars/joda-time-1.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/joda-time-2.1.jar:/opt/cloudera/parcels/CDH
-5.10.0-1.cdh5.10.0.p0.41/jars/jopt-simple-4.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jpam-1.1.jar:/opt/clouder
a/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jsch-0.1.42.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jsp-api-2.1.jar:
/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jsr305-1.3.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jsr
305-3.0.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/jta-1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41
/jars/junit-4.11.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kafka-clients-0.9.0-kafka-2.0.2.jar:/opt/cloudera/parce
ls/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kafka_2.10-0.9.0-kafka-2.0.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kite-d
ata-core-1.0.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kite-data-hbase-1.0.0-cdh5.10.0.jar:/opt/cloude
ra/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/kite-data-hive-1.0.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/j
ars/kite-hadoop-compatibility-1.0.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/leveldbjni-all-1.8.jar:/op
t/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/libfb303-0.9.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/libt
hrift-0.9.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.
0.p0.41/jars/log4j-1.2.17.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/logredactor-1.0.3.jar:/opt/cloudera/parcels/CD
H-5.10.0-1.cdh5.10.0.p0.41/jars/lz4-1.3.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/mail-1.4.1.jar:/opt/cloudera/p
arcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/mapdb-0.9.9.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/maven-scm-api-1.4.j
ar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/maven-scm-provider-svn-commons-1.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1
.cdh5.10.0.p0.41/jars/maven-scm-provider-svnexe-1.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/metrics-core-2.2.0.j
ar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/metrics-core-3.0.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41
/jars/metrics-json-3.0.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/metrics-jvm-3.0.2.jar:/opt/cloudera/parcels/CDH
-5.10.0-1.cdh5.10.0.p0.41/jars/microsoft-windowsazure-storage-sdk-0.6.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/
mina-core-2.0.4.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/mockito-all-1.8.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1
.cdh5.10.0.p0.41/jars/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/netty-3.9.4.Final.jar:/opt/clou
dera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/netty-all-4.0.23.Final.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/op
encsv-2.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/oro-2.0.8.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.
41/jars/paranamer-2.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-avro-1.5.0-cdh5.10.0.jar:/opt/cloudera/par
cels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-cascading-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars
/parquet-column-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-common-1.5.0-cdh5.10.0.jar:/opt/
cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-encoding-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/parquet-format-2.1.0-cdh5.10.0-javadoc.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-format-2.1.0-
cdh5.10.0-sources.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-format-2.1.0-cdh5.10.0.jar:/opt/cloudera/parce
ls/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-generator-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/p
arquet-hadoop-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-hadoop-bundle-1.5.0-cdh5.10.0.jar:
/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-jackson-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.
10.0.p0.41/jars/parquet-pig-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-pig-bundle-1.5.0-cdh
5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-protobuf-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.
10.0-1.cdh5.10.0.p0.41/jars/parquet-scala_2.10-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-s
crooge_2.10-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-test-hadoop2-1.5.0-cdh5.10.0.jar:/op
t/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/parquet-thrift-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0
.p0.41/jars/parquet-tools-1.5.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/pentaho-aggdesigner-algorithm-
5.1.5-jhyde.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/plexus-utils-1.5.6.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cd
h5.10.0.p0.41/jars/protobuf-java-2.5.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/regexp-1.3.jar:/opt/cloudera/parc
els/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/scala-library-2.10.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/serializer-2.
7.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/servlet-api-2.5-20110124.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5
.10.0.p0.41/jars/servlet-api-2.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/slf4j-api-1.7.5.jar:/opt/cloudera/parce
ls/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/snappy-java-1.0
.4.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/spark-1.6.0-cdh5.10.0-yarn-shuffle.jar:/opt/cloudera/parcels/CDH-5.
10.0-1.cdh5.10.0.p0.41/jars/spark-streaming-flume-sink_2.10-1.6.0-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/j
ars/stax-api-1.0-2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/stax-api-1.0.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1
.cdh5.10.0.p0.41/jars/stringtemplate-3.2.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/super-csv-2.2.0.jar:/opt/clou
dera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/tempus-fugit-1.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/trevni-a
vro-1.7.6-cdh5.10.0-hadoop2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/trevni-avro-1.7.6-cdh5.10.0.jar:/opt/clouder
a/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/trevni-core-1.7.6-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/
twitter4j-core-3.0.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/twitter4j-media-support-3.0.3.jar:/opt/cloudera/par
cels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/twitter4j-stream-3.0.3.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/unused-1.0
.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/velocity-1.5.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/j
ars/velocity-1.7.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/xalan-2.7.2.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5
.10.0.p0.41/jars/xercesImpl-2.9.1.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/xml-apis-1.3.04.jar:/opt/cloudera/parc
els/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/xmlenc-0.52.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/xz-1.0.jar:/opt/cloude
ra/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/zkclient-0.7.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/jars/zookeeper-3.4.
5-cdh5.10.0.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/LICENSE.txt:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10
.0.p0.41/lib/hadoop/NOTICE.txt
DEBUG Client -         SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1488308813765_0068
DEBUG Client -         SPARK_YARN_CACHE_ARCHIVES_FILE_SIZES -> 36735
DEBUG Client -         SPARK_USER -> pl24116
DEBUG Client -         SPARK_YARN_CACHE_ARCHIVES_TIME_STAMPS -> 1489695291010
DEBUG Client -         SPARK_YARN_MODE -> true
DEBUG Client -         SPARK_YARN_CACHE_ARCHIVES_VISIBILITIES -> PRIVATE
DEBUG Client -     resources:
DEBUG Client -         __spark_conf__ -> resource { scheme: "hdfs" host: "nameservice1" port: -1 file: "/user/pl24116/.sparkStaging/
application_1488308813765_0068/__spark_conf__1833562380849170421.zip" } size: 36735 timestamp: 1489695291010 type: ARCHIVE visibilit
y: PRIVATE
DEBUG Client -     command:
DEBUG Client -         LD_LIBRARY_PATH="{{HADOOP_COMMON_HOME}}/../../../CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hadoop/lib/native:$LD_LIBRA
RY_PATH" {{JAVA_HOME}}/bin/java -server -Xmx512m -Djava.io.tmpdir={{PWD}}/tmp -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPe
rmSize=256m org.apache.spark.deploy.yarn.ExecutorLauncher --arg '10.3.112.22:43512' --executor-memory 1024m --executor-cores 1 --pro
perties-file {{PWD}}/__spark_conf__/__spark_conf__.properties 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
DEBUG Client - ===============================================================================
INFO  SecurityManager - Changing view acls to: pl24116
INFO  SecurityManager - Changing modify acls to: pl24116
INFO  SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(pl24116); users
 with modify permissions: Set(pl24116)
DEBUG SecurityManager - Created SSL options for fs: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None,
 trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
DEBUG Client - spark.yarn.maxAppAttempts is not set. Cluster's default value will be used.
INFO  Client - Submitting application 68 to ResourceManager
DEBUG Client - The ping interval is 60000 ms.
DEBUG Client - Connecting to lbdp005a.rnd.pncint.net/10.3.112.24:8032
DEBUG UserGroupInformation - PrivilegedAction as:PL24116@PNCBANK.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.se
tupIOstreams(Client.java:726)
DEBUG SaslRpcClient - Sending sasl message state: NEGOTIATE

DEBUG SaslRpcClient - Received SASL message state: NEGOTIATE
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
  challenge: "realm=\"default\",nonce=\"yxZWXf03Alx7W7widQw2oeOabBMpJQkGezBTh2vB\",qop=\"auth\",charset=utf-8,algorithm=md5-sess"
}
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "yarn"
  serverId: "lbdp005a.rnd.pncint.net"
}

DEBUG SaslRpcClient - Get token info proto:interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB info:org.apache.hadoop.y
arn.security.client.ClientRMSecurityInfo$2@102ee79e
DEBUG RMDelegationTokenSelector - Looking for a token with service 10.3.112.24:8032
DEBUG SaslRpcClient - Get kerberos info proto:interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB info:org.apache.hadoo
p.yarn.security.client.ClientRMSecurityInfo$1@77683e5e
DEBUG SaslRpcClient - getting serverKey: yarn.resourcemanager.principal conf value: yarn/_HOST@PNCBANK.COM principal: yarn/lbdp005a.
rnd.pncint.net@PNCBANK.COM
DEBUG SaslRpcClient - RPC Server's Kerberos principal name for protocol=org.apache.hadoop.yarn.api.ApplicationClientProtocolPB is ya
rn/lbdp005a.rnd.pncint.net@PNCBANK.COM
DEBUG SaslRpcClient - Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at lbdp005a.rnd.pncint.net
DEBUG SaslRpcClient - Use KERBEROS authentication for protocol ApplicationClientProtocolPB
DEBUG SaslRpcClient - Sending sasl message state: INITIATE
token: "`\202\t\275\006\t*\206H\206\367\022\001\002\002\001\000n\202\t\2540\202\t\250\240\003\002\001\005\241\003\002\001\016\242\a\
003\005\000 \000\000\000\243\202\b\322a\202\b\3160\202\b\312\240\003\002\001\005\241\r\033\vPNCBANK.COM\242*0(\240\003\002\001\000\2
41!0\037\033\004yarn\033\027lbdp005a.rnd.pncint.net\243\202\b\2060\202\b\202\240\003\002\001\027\241\003\002\001\003\242\202\bt\004\
202\bp2\3503\202\211\216\250\232\266;\347\257\261\306\242E\344h\254\367\263\023\323D~\004Z.\214\321\257n3\232\337Z\201\022\320\335\3
00K+\363\244\216\334;\200[H\223\000G\230\212H\327\216s9\365\214\270@\350\332\016\'2\"\034J\222J\276\021\231\307\373\337\260\203\275\
267m\233\362\377\bJ\270\274`\032\031\234\031\f~\230\203n\023\367j\272\335\365\260FN\2076=\316\210\207\035\3042\'\357\300\227\025\204
=Y^\364\034X3\366\365\276\324v\026\345Y\032\332\342\027\252\225\251L\206y\233\023\234?\357@M\367\344\337\242=r\2237\270\246\260\031v
\306\343\217\217O\211t\033\220\375\026B\276\336\272\004]&\360\240\3530\235\363&0\271\356;\305\255`\036\026\230U\321\200\335\217\233\
230\313\234\222\327\345\302+8\275\303\233x\205};\347\345\b\220\253\273P\213(oDI_\032F-\027#\360\3469h\340\264\037\365foQ\v<\251p\321
/\370Y\274\242\373\034\367\316B\231\315\313\331\\\275\377\nz\f\201\200\021\325\337\232Jm\223\270{\271\307\331\200\334\005A\310.\n\30
3\243\004%\337\232a(\031\360\v\312\\\032_\366\244:T/TS\357CO7\266\353\205\237\317\017NZ\334\033\023\303,\350/\36009\372\n\322\221\36
1YZ|i\347\233\275#\242\311X!a\371\267\356O=\307\f\005 \373\023\242#\347\377U\005\3040\323\336\361\023\024\a\310\234\237\204j\005\bc\
016\217\213)P\251\t\246\226\245\264\314a\310\2142%v\n5\322\261\217\371S\241\365\2564M4\302\371E\334\246\006q_\247\254Jt}\2009{\347\2
47\271B\200\206\316\272~\265\351#\301O5\'\324\030\351% \3679\224\254\365\313t\305U\227u\"\246F.\333\301*\002\372\262\335\307\0177Ge>
\330\222Q%\\\0305\217Q\311\315\273\377n\306\224\352\330\v\350\312n\231\220\002S\243@\023wb\030\213\324;0\325\001\263\312o\200\235Z8\
342\016L\337\367k?\325\307cU\335Z\253\274\006\272\027\002\025g`\351\035\200\335]1\206$\267L\261@\026\031\277\033\377\206\276a\026\24
2\355a\262\213\020%\215\366\344\352\322[\277%\231\025H\212\277\341.(\245ls\26245\037\021\215@.:*\217q\256\262~\300\265\236\"\034\241
\026\004p\004~g\352\365\326\306\303yX=\241\312\301\372{=Ww\312u\207\031\fD\357\'\313\330\315\225\221a\333~\361\241\025\371\3601N2\37
7\244\256\373`\357\'1ZE{\024\334\234\201J\026\032\024w\252\"q\233\257\336\265q\237\331W\361\345\252\372 \262\200C7uQ|\244\337\262\31
4\223\273\302\342\213\205\320\260\226\201\226><\333x\274\343L\227A\307\244\000\301\315P\301\264?\362\303\201[0\240:B\342\376\a(k\a6\
313\260\000DdRI\211\257\037\255.\375sI\231\223g8qE\na\205\365,RA\332X\027\322\270[i\354\320Y\v\3043\336U\024L\020\304,\226@_M3\v\372
\307\343b\t\232E3BEc\217\232\224\336\306\322\234\326\273\334\356\276\375\363\331_\032\317,\324\211\362\204q\326\306Xa$o\020\354%\314
\276(\307\225\005+\342\341\266\257\355lkT\034\362\371\214\b\001\222\327\334\352\311\210\037&\254Q\370\363Yio\246d\371i0\302\336\"a\0
30\307\201w\004y\374\351#\254\345r\006y\314h~\345\033\223\267v\251\242\337\374_PZ\330\025\023\351\246KQ\345\245-\020K\367\270\\\v\20
2FK\3154\322\003\364B\217edj\365f!\037\323S\246\366\250\377\366\025 -\000-C\271\222\345d\\\217O\216>K\200\371y\030\272\b%[\257\327\2
72+\206>\341\253\025\356\364\212Ayb\353_l\030\002SJ\337;x\262y\353:\004\035\344\243\360\030?\316\277?5\302\256^\3241o\217jk\024Z5|\3
03+\317\r\236\31403mr\022<<\323\302CJ\340@.\023\346Af\277\2764+\205I\320h\325a}\266\317\3447\332\317\023\3705\305PUY\271HA\330-\347\
307\232w\026\367\201\372\213\371\303[\362^\3156\342G\214_\364D\306\270\206\246\2303\020\312\034%\021lvg\340\314\232\bM\337\330*\260u
vq\304\343\250\030f\272r\263\3731e\002\036\034q\260\354\334\344\350;\373T5\025\220\212\'\363Jj\221h\027\004\303G\301\024\326%`\004F\
340\346h\360\231V2>\t\024\304/\247\233E\'v[\347\267\265p}\253G\2223u\030\355\034\vFz\3502\201\243\337\030\000\206\271\344\242=y\336-
\227\321\237\224\3729\344\311\217\n\255|\002\346\373\255\033m\200\321N\335\3353l\304/\237\345a\264\273#\375\031\205|\251@o\305h~\364
]\030\315wD;\220T66(\316\317\315E{`\017\227\223\016\016\304\253}js(\256\002H\037j\005_S3\255ps\315\227\346\206r8[p\272vM\337M!\275g\
330\202\366\206\225v\247\275F\233\037\024\025\027QN\262\247F)\005d\340\311\0234i\313\332Sg\3001\000\322#OdT,Z\206\\\\\213\300?F\022C
c\262e\270\213\\\214\020\035\365\227\314\216%Q\362B\261\205\303{\265S\315\376vr\2606\f\340(\bz\346\262\006\235\330\327\004k<\217s\32
0\006?\364\272AtT\352\225\3473\346qS\017\256\000\261\332\017X\356\315\227\003\332:\037c\206\254\320Fj\204\367\365\001b7\303\264zN\30
2\203\035v\346\322\251\024\022X\020 y\373)$\341M\001\002\235\254\340\343\021x\314\0370a(W\315\027A^}\200\002|\023\025k\t\365\336\376
\017\245\330\301\317V(H\005\206\t\357\302\203c0#\251\022\244\226d\006\276\203\240g\310\315\246+\226\302dd\222J_}\211\376S\033l\034\3
26\214\362\336Q\364<`\003\264\255\257$\022\021\313Q^\024c\202\006;\351\244!Xc\320\273\356DQ\2713\000\000\363\267\035H\336^\025\000 \
rM@Yz\025?\r\002\253\222>\026\344H{9c\376@\362\224\341\242\232\034\312\002\374\036 \032g\033Q\031\236$\325QG\246r\366\321u\214\335nb
\340\214\345b\224^o\317\224+\034\360mH\265\345\255\372\246\207\t*\a\312\004\304\257\035\341\216Q1\245t)\266\030]\223\036\200\315\356
\262\025O7\017h\002\031xZ\205J\256\036l\n\335\244\323d<\217\340\215\256.TC\2141\206\31252\314\341\257\233U!\035?\373\261\205\245\246
9&L\266b\217#\'0\276=\000\257\322!\300\247\a=\327f\'TCmN\243!\352%\\\364\303\222b<\253\250bq\366BU\262{\240\034BSl\215\232\334\235\3
12\365\376*5\337b\216\a&\300Z\224\241$p\233M\350\266sn\270\344\330\354\024\272\333\323`\003%,\253O\000.a\232\234\271\315\005di\3170\
250\2463\205,\243.\222\023\357\266\023`.\273\210<,W\230\370]\377\214(e\0046\234\260E\255E\314\304h\027\271\342\032\365\303;\021\323\
255\340\362\302\204\276\304\345\307D\245\217!\213\362\"\230\305\243\253;N\226\025\271L^e\340\362\224\320@(o\b\236\3020\376;\020\001\
312\243q\264\347\2772\216\031\210\355\310\"\t/\304\377\300@X\330\320[\350?\v\330\233S\202\205\223\254d\374\361\267(&\207:\362\345\33
5\301fpR8\021\266d]jHdj\311\036a\b\002\327\266\202\35575\376\272\240\023\t \246\033_\314]\350\033\216\25277cc\270C|\226\371H\343\360
\266L\217O\352\335\215\323U[\252\302Z5R\277\275\327E5VS5N\034\312J\363\350\35235\332\210\376\252H\227\323\2440g\336\330\216G\017\332
C\351qJ`\236\234\000\371\377\215\a\243\301m\t8!z\344\247Xj]\3741\277\317\351\215\213\251#\376e)\r\353\337\267\336~\225.\200J\247\205
\351\027\244\201\2740\201\271\240\003\002\001\027\242\201\261\004\201\256\353\017\340\254\273\347\0232\351\303\017\257\036g\345w\267
\271\343\274\340\205\245\324\v4\275\332\034\345\246\2378\207\3641\\\302\235\027h\031\276\266\333L/ZT\223\3638M(\260<\367\002xM<\035;
d\302\245\024\324\311\317\"\371\003O\354\030\354\352\314B\227\233\305\273\365za\354\213p\260J\365\240\302uX\377\000\325\301\304\333o
\237\205\036}\303Q\274q?\225\213Dw\023bGE/\315\300\r\237\035%S\206+?\247B<\237\302\026Kp`\204-\223tI\220\230]\253Z\337\320\264\342{A
\0319\247%E\372\003\'J\240\216\343G~\314\t\351"
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "yarn"
  serverId: "lbdp005a.rnd.pncint.net"
}

DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM: closed
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM: stopped, rema
ining connections 1
DEBUG SaslRpcClient - Received SASL message state: CHALLENGE
token: "`f\006\t*\206H\206\367\022\001\002\002\002\000oW0U\240\003\002\001\005\241\003\002\001\017\242I0G\240\003\002\001\027\242@\0
04>E\256\203\370\375,W\'<aID\260\204|dv\377\277\"\273c\363>B|\f\343\222\261\'(\361B?\345\250 \265\252.\005j-\245\245\303\273?\032F\2
43\241\232\033\272\t\227\354\235j\016"

DEBUG SaslRpcClient - Sending sasl message state: RESPONSE
token: ""

DEBUG SaslRpcClient - Received SASL message state: CHALLENGE
token: "`0\006\t*\206H\206\367\022\001\002\002\002\001\021\000\377\377\377\377\f\251P\344l\347\300x\300fd`IU\260\2236\"Z9\347F\327\2
66\001\001\000\000\001"

DEBUG SaslRpcClient - Sending sasl message state: RESPONSE
token: "`0\006\t*\206H\206\367\022\001\002\002\002\001\021\000\377\377\377\377\000_\333-k\357\265\027\363cQ\365m%\221X\301\275;,\354
\300R\350\001\001\000\000\001"

DEBUG SaslRpcClient - Received SASL message state: SUCCESS

DEBUG Client - Negotiated QOP is :auth
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM: starting, hav
ing connections 1
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #16
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #16
DEBUG ProtobufRpcEngine - Call: submitApplication took 1892ms
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #17
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #17
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 5ms
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #18
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #18
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  YarnClientImpl - Submitted application application_1488308813765_0068
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #19
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #19
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 2ms
INFO  Client - Application report for application_1488308813765_0068 (state: ACCEPTED)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #20
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #20
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  Client - Application report for application_1488308813765_0068 (state: ACCEPTED)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #21
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #21
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  Client - Application report for application_1488308813765_0068 (state: ACCEPTED)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #22
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #22
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 2ms
INFO  Client - Application report for application_1488308813765_0068 (state: ACCEPTED)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #23
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #23
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  Client - Application report for application_1488308813765_0068 (state: ACCEPTED)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #24
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #24
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 2ms
INFO  Client - Application report for application_1488308813765_0068 (state: ACCEPTED)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #25
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #25
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  Client - Application report for application_1488308813765_0068 (state: ACCEPTED)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: N/A
         ApplicationMaster RPC port: -1
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
DEBUG ResourceLeakDetector - -Dio.netty.leakDetectionLevel: simple
DEBUG Recycler - -Dio.netty.recycler.maxCapacity.default: 262144
INFO  YarnSchedulerBackend$YarnSchedulerEndpoint - ApplicationMaster registered as NettyRpcEndpointRef(null)
INFO  YarnClientSchedulerBackend - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS ->
lbdp005a.rnd.pncint.net,lbdp014a.rnd.pncint.net, PROXY_URI_BASES -> http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813
765_0068,http://lbdp014a.rnd.pncint.net:8088/proxy/application_1488308813765_0068), /proxy/application_1488308813765_0068
INFO  JettyUtils - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@52f1234c + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@52f1234c + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4ee8ff06}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4ee8ff06=org.apache.spark.ui.JettyUtils$$anon$2-4ee8ff
06}
DEBUG AbstractLifeCycle - starting org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f
DEBUG Holder - Holding class org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
DEBUG AbstractLifeCycle - STARTED org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4099dde9 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4099dde9 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-df9d3bc}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-df9d3bc=org.apache.spark.ui.JettyUtils$$anon$2-df9d3bc
}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@35b80d21 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@35b80d21 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-4c12f0f6}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-4c12f0f6=org.apache.spark.ui.JettyUtils$$anon$2-4c12f0
f6}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@46660d14 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@46660d14 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-6f89cd7e}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-6f89cd7e=org.apache.spark.ui.JettyUtils$$anon$2-6f89cd
7e}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@55e4c027 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@55e4c027 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-6f48e70d}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-6f48e70d=org.apache.spark.ui.JettyUtils$$anon$2-6f48e7
0d}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@1d036c7f + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@1d036c7f + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-1b393f8f}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-1b393f8f=org.apache.spark.ui.JettyUtils$$anon$2-1b393f
8f}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4bc814ba + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4bc814ba + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-2e69f3d0}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-2e69f3d0=org.apache.spark.ui.JettyUtils$$anon$2-2e69f3
d0}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@3cc8e634 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@3cc8e634 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-2e05f47e}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-2e05f47e=org.apache.spark.ui.JettyUtils$$anon$2-2e05f4
7e}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@29817f19 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@29817f19 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-74617c53}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-74617c53=org.apache.spark.ui.JettyUtils$$anon$2-74617c
53}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@74bf5798 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@74bf5798 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-2c2968d6}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-2c2968d6=org.apache.spark.ui.JettyUtils$$anon$2-2c2968
d6}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4c1bf0b9 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4c1bf0b9 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5942c2af}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5942c2af=org.apache.spark.ui.JettyUtils$$anon$2-5942c2
af}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@3e5928b8 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@3e5928b8 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-2dfb9cf1}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-2dfb9cf1=org.apache.spark.ui.JettyUtils$$anon$2-2dfb9c
f1}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4f806802 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4f806802 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-12efc256}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-12efc256=org.apache.spark.ui.JettyUtils$$anon$2-12efc2
56}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@772c8414 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@772c8414 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-3abfa5d0}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-3abfa5d0=org.apache.spark.ui.JettyUtils$$anon$2-3abfa5
d0}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@b937a2c + org.apache.hadoop.yarn.server.webproxy.amfilter
.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@b937a2c + [/*]/[]==31=>org.apache.hadoop.yarn.server.webp
roxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-7b0f62cb}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-7b0f62cb=org.apache.spark.ui.JettyUtils$$anon$2-7b0f62
cb}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@7f39e4a3 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@7f39e4a3 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-2f1f3c8b}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-2f1f3c8b=org.apache.spark.ui.JettyUtils$$anon$2-2f1f3c
8b}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@31d1ca96 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@31d1ca96 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-3c97d96c}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-3c97d96c=org.apache.spark.ui.JettyUtils$$anon$2-3c97d9
6c}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@444b5767 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@444b5767 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-15436088}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-15436088=org.apache.spark.ui.JettyUtils$$anon$2-154360
88}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@2e574e38 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@2e574e38 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-7cc8b288}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-7cc8b288=org.apache.spark.ui.JettyUtils$$anon$2-7cc8b2
88}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@50d1cbcc + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@50d1cbcc + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-5c413eb8}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-5c413eb8=org.apache.spark.ui.JettyUtils$$anon$2-5c413e
b8}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@7d9ae787 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@7d9ae787 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.spark-project.jetty.servlet.DefaultServlet-17b45674}
DEBUG ServletHandler - servletNameMap={org.spark-project.jetty.servlet.DefaultServlet-17b45674=org.spark-project.jetty.servlet.Defau
ltServlet-17b45674}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@6c3355f2 + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@6c3355f2 + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-5c35d5da}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-5c35d5da=org.apache.spark.ui.JettyUtils$$anon$3-5c35d5
da}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@1e4fba5d + org.apache.hadoop.yarn.server.webproxy.amfilte
r.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@1e4fba5d + [/*]/[]==31=>org.apache.hadoop.yarn.server.web
proxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/*=com.sun.jersey.spi.container.servlet.ServletContainer-6b1bb196}
DEBUG ServletHandler - servletNameMap={com.sun.jersey.spi.container.servlet.ServletContainer-6b1bb196=com.sun.jersey.spi.container.s
ervlet.ServletContainer-6b1bb196}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4543f37 + org.apache.hadoop.yarn.server.webproxy.amfilter
.AmIpFilter-6b81b95f as filter
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@4543f37 + [/*]/[]==31=>org.apache.hadoop.yarn.server.webp
roxy.amfilter.AmIpFilter-6b81b95f as filterMapping
DEBUG ServletHandler - filterNameMap={org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f=org.apache.hadoop.yarn.ser
ver.webproxy.amfilter.AmIpFilter-6b81b95f}
DEBUG ServletHandler - pathFilters=[[/*]/[]==31=>org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f]
DEBUG ServletHandler - servletFilterMap={}
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$3-7259f1b0}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$3-7259f1b0=org.apache.spark.ui.JettyUtils$$anon$3-7259f1
b0}
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #26
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #26
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 77ms
INFO  Client - Application report for application_1488308813765_0068 (state: RUNNING)
DEBUG Client -
         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
         diagnostics: N/A
         ApplicationMaster host: 10.3.113.22
         ApplicationMaster RPC port: 0
         queue: root.users.pl24116
         start time: 1489695302929
         final status: UNDEFINED
         tracking URL: http://lbdp005a.rnd.pncint.net:8088/proxy/application_1488308813765_0068/
         user: pl24116
INFO  YarnClientSchedulerBackend - Application application_1488308813765_0068 has started running.
DEBUG TransportServer - Shuffle server started on port :36031
INFO  Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36031.
INFO  NettyBlockTransferService - Server created on 36031
INFO  BlockManager - external shuffle service port = 7337
INFO  BlockManagerMaster - Trying to register BlockManager
INFO  BlockManagerMasterEndpoint - Registering block manager 10.3.112.22:36031 with 530.3 MB RAM, BlockManagerId(driver, 10.3.112.22
, 36031)
INFO  BlockManagerMaster - Registered BlockManager
DEBUG ServletHandler - filterNameMap={}
DEBUG ServletHandler - pathFilters=null
DEBUG ServletHandler - servletFilterMap=null
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-35c91146}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-35c91146=org.apache.spark.ui.JettyUtils$$anon$2-35c911
46}
DEBUG Container - Container org.spark-project.jetty.server.handler.ContextHandlerCollection@34849e52 + o.s.j.s.ServletContextHandler
{/metrics/json,null} as handler
DEBUG AbstractLifeCycle - starting o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@6c14d50c + org.apache.spark.ui.JettyUtils$$anon$2-35c9114
6 as servlet
DEBUG Container - Container org.spark-project.jetty.servlet.ServletHandler@6c14d50c + [/]=>org.apache.spark.ui.JettyUtils$$anon$2-35
c91146 as servletMapping
DEBUG Container - Container o.s.j.s.ServletContextHandler{/metrics/json,null} + org.spark-project.jetty.servlet.ServletHandler@6c14d
50c as handler
DEBUG AbstractLifeCycle - starting org.spark-project.jetty.servlet.ServletHandler@6c14d50c
DEBUG ServletHandler - filterNameMap={}
DEBUG ServletHandler - pathFilters=null
DEBUG ServletHandler - servletFilterMap=null
DEBUG ServletHandler - servletPathMap={/=org.apache.spark.ui.JettyUtils$$anon$2-35c91146}
DEBUG ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-35c91146=org.apache.spark.ui.JettyUtils$$anon$2-35c911
46}
DEBUG AbstractHandler - starting org.spark-project.jetty.servlet.ServletHandler@6c14d50c
DEBUG AbstractLifeCycle - STARTED org.spark-project.jetty.servlet.ServletHandler@6c14d50c
DEBUG AbstractHandler - starting o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$2-35c91146
DEBUG AbstractLifeCycle - STARTED org.apache.spark.ui.JettyUtils$$anon$2-35c91146
DEBUG AbstractLifeCycle - STARTED o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG Client - The ping interval is 60000 ms.
DEBUG Client - Connecting to lbdp014a.rnd.pncint.net/10.3.112.25:8020
DEBUG UserGroupInformation - PrivilegedAction as:PL24116@PNCBANK.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.se
tupIOstreams(Client.java:726)
DEBUG SaslRpcClient - Sending sasl message state: NEGOTIATE

DEBUG SaslRpcClient - Received SASL message state: NEGOTIATE
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
  challenge: "realm=\"default\",nonce=\"V5Yo9GIIVdXibeRVeJHn/xHDD7cMasqpOUFEVlrE\",qop=\"auth\",charset=utf-8,algorithm=md5-sess"
}
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "lbdp014a.rnd.pncint.net"
}

DEBUG SaslRpcClient - Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.had
oop.security.token.TokenInfo(value=class org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector)
DEBUG SaslRpcClient - Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.
hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
DEBUG SaslRpcClient - RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB i
s hdfs/lbdp014a.rnd.pncint.net@PNCBANK.COM
DEBUG SaslRpcClient - Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at lbdp014a.rnd.pncint.net
DEBUG SaslRpcClient - Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
DEBUG SaslRpcClient - Sending sasl message state: INITIATE
token: "`\202\t\275\006\t*\206H\206\367\022\001\002\002\001\000n\202\t\2540\202\t\250\240\003\002\001\005\241\003\002\001\016\242\a\
003\005\000 \000\000\000\243\202\b\322a\202\b\3160\202\b\312\240\003\002\001\005\241\r\033\vPNCBANK.COM\242*0(\240\003\002\001\000\2
41!0\037\033\004hdfs\033\027lbdp014a.rnd.pncint.net\243\202\b\2060\202\b\202\240\003\002\001\027\241\003\002\001\003\242\202\bt\004\
202\bp\207\236\246\"\231u\3140\233\002\303\247\210\255\247@\003HF\244#2\233\370\213\342\362\260w\217\265\270w%\306|\t\236\333@m\241\
273u\241Gf\032\255\024\023\004\374TXC\266\252\n\3026x\026v\224I\275\037eQ\313uN(\371\0013\3538\217k\204\251\326\021\a\243^\302\006\0
04\351\323\032F\364&6\025\033\362\274\bv\237\242\253\262\354\304B.-\325\316*\247\376Wz\223\301\224\200\354\036\361\b_\314\225\336]\2
44\264\214\353i\202\302\323D\017\000\215VQ\200p\310\376\250\324\372\"\0367m\255tt?\356\310\036m\237\\\203H\004\375\334H;\221\342f+$\
354c\276\360\355\352\341\234kx\3668.8/\370\t\202 \310Z\216Z*\271\260\344\022\200\254L\021\302\035\364\375\a\225s\2465\016J-\231\201\
310i\342\366\034\000!X\244\317\257\342\243\277d\243\314\260\230,\335Sv\225\304\016\031\361\024/X4\245\016\270\n\036xX\353\221\225.k8
M\372\211:l\331\003\005\331)\234G7\300\344\0219i\b\277\242#U\355!\034\236u\220>SB\255\356\371\224\'\340\273\244\376\341\314\361\032\
235\254\324\304\244\2364x\235\356\000Vt\362C6_\2170\327R\t\300c\353\377\336\352P=\022F\016\256\234\036ee\216>Ia{b\035a\361cZe\342\22
4V\025GiX\340~\274<\270auP\233\274\314\231CA\034\306\202-\2742&\036\f\241\275\000\326\312\254R\np/\034\255\224\b\204\357\\\v.\231\30
5\231N\304\266\021)\351\003\242H\002\313\340\215K\026\"U\032_\030\367\315h\342\267\023w4M\023\312F\265\211Y\347\363J\276n\357\255K_\
366\020k\356D\270\202#\021p\336\bo\036N\316 6\346\347a4#\262\376\006\300\t\211\205\'[$A\v\033:!6U\200\305r\344U\332\273\235\215\253\
311\261q\r\217\227rE\205X\320\371\305\'\253:\331\027\261\v\276\214Y\236yI5\247_\204\240\002\352\031\316\207\263~\312j\343\322\270\00
6[\027v_\354\027\310\0307f\377g\224\362\224E\35273\bU\303\302\2171hID\225\345\250\022\203 \303\261\034k6\202\350\345\030\302\332\220
\"-\334\265x\304 \v\f\331\314\337\270\342\306\335\270\225\355\325h\016\256*\210\252\267\274\r\324\023\021S\223s\253F7\354\375eR\237\
200X6\243\254kr\214\305\376\220\017\021)\016\245\223\372\252\221\027Bf\340\270=Kd\306\023+0\302\352*\317\221\361\246H\261\340\002\26
1\373\352\0253%\247\312o\362\240\306\312\202\2167\201h&\304`\313\242Y:o\337\307\340\365sl[\232\347\031\260\276\316\344_u\005\222l\36
2\f^\275\005$m\347\t\340}\0263P\3023O\223z7\314\\Y9\353\307@5\324\337\335Z\213\344,\235\347\372iU\342\253\2310\216r\243\326\236\313\
231}\317\204\022d\255D{\271y\305\217L~\234\b\323C\3447\036\366Z\336\304\v&\262\227\254\212\rhhf#\330\353\264\315\231.e\206z\337\325\
364L\275\327\323-&h\254\037\304\252\031$\236\265\207\366\246\350\207\370\226E \345\345?\247\fmOa\"k\355\366\3465\254q\n\001\324\023\
277\374\366\351U2\233\240\334\315\260\330\312\346\307`\200_3\212a\255\370\201cQ\243+*\351\ng\236\234Y\334D\016\200\212RP\366]\354\31
1V\270\233\'\305\0275\000b\240\270f\262`\035D\325\225\367\346\005*\257-\217R\347\277\373\375h\255\374\036>\333k\tp\321\0308_\3017O9\
215~\232\314\340\267\274\340F\304\n\200\027\323\360\355\336\235\321WB\b\251\212\"1\226\361kKcD\277\217/^|J\365\210b\252\254\357\030v
\371\330P\311{\306\225\343\377\2178\250\266\375)\360\v\333\023\212\321\2348UX\216Q%\375j\205\323\034\300T\243\277\032\241\035p3i-\37
7sX\201\317of\217\313\t\200\347i\221\\\326\223\350\347$\214R\266\000\264-\325\210,\275)\215W\216NF\2469\027!\\\350c\237=m\375\316/\3
26g\023\333\371T\032\\\222\264\032\352\366}L`\311\237\212k\f\307e\246\260\301\036:z\026\264\323/3\270\322\342\202\264\r\035<\"F\361\
272\337\270GeL+\252\016\"\260b]\343\271Z\362\217&\235E\256\024\340\262YD\357DG\340\240\236\322I\277O\362\306\300>\207z\205/\373dM\02
0\343K\2103\377nT\034\344@LF1G\250\211\257a\265\305x\005\356\310\276\330\f\262q\020Z!$V\237FN3\023\261\320\033\200ky\310\370\207C\35
0BI\246\2649!Nu\210\306\247\325\b\357\017\003\335\343\2450o\032{\352\317\252\377mm?\311 \236\312\330E\216?-\213,Q\253\034\337C\261\2
41\352D\342\235\224@yn\362\246$\213\225\321ap\027\b\016%\212\263\320\'\237\006c\240\034\216=\2366HgE\025p\022\355\265TP\001\221\020\
370\301Fc\301[\315\274\205\316t\340\370r\031\257aC\362\235(\025\240\354)\224\234uP\037\026#\362C{Tz\355c\253\372\350\2022\005\345\25
6\033v\323\345\264\255\221\306S\305\033*\371+\006\354\331R+,I\365(\"D\247\372\025\'Ta\272\276S\"Z\272\361\226\354\204h\235B\204\030D
\024?\332X\254\272/\322\274\215\357\237x\2202\345(\237Ew\334\005.\260\253\024\f\265~\3707Cy\200:Z\a\021;i}5p==p\33688C\204\211\345\3
51\021\3160&\223(b\306\222L4ps\273\026\003jqS\365<jR\t*<fw\343\234\265\324\335X@\335bw\036U\330]ssr\224\306\274%\307\2256kuF\346?\37
4\253\000\"Z?\031}\343 \230M!\260y\246\vd\275.\254>g\034\345\343\366\352\315OG\326\243\304y\323\242\355\312\305\204\350z\372\0236\26
3\363\262\357\253(+d\036\232\221M\364\f\263\310>\367\031\247\276\375\265\022\035\310\223\0162c\034\267<o\355\004^\n_N\321z\274 \223T
\235\242#h\241\341H\221t\vQ\216\272g&\245\033\245\363\271\204\216\231\274\a\367\031\260R\204\301\320Qlg#\364\227\362O\033\t\300!\256
\234\276Ty\207\266\211d\240\027\3240\f\344\217d=\315\334N/\337ey\332\314\t7\274jY \364\276\250\362C\222\303\022\326N^]>-?\214\317\v\
027\022D\217Qi\2435\226\234\300\302s\025\224Q\210\206\034\241\370Y\365\205NS\3112\343`\021\226\340\374rNl\243\253\020Q\235\026\354\3
51\0238\003\265\340\245\205&\212\276\343\003\236\252=#=b\000\242\252C\361/\220\004n\241\256\034\374\031\307\241\242\376<\vJ}\257\327
\t\375\377\032\242{B\353\216pO\317[0EL\3435\"AI\032\rkM<\'\016\027\343\247\236\325\3363\377J\376\264K\3665\020\311\364\006\233Q\020\
340\365Rs\231\224$\252\246\'*o\335R[\364\230:\264xn\327\036=\361\032\n\310@\311\034\311\241\243K\334kP\270\304_v|g\347\235(\323dhpf\
207\335\355\233!\310\272\226\336y\021\275\325\276\321k>\226t\000#\344J;;ic}\261\033s\235\316#\226\026$\245\202\245Gg\021\363p\356\02
3%),s~NI\365\203)\303(\030\345\362\316*\375\326~\354R\252\201`\320\347\3038\242\253\326thg\376\351\303\316\204U\235\364\346\b-G\244\
316\206\335\336\223\2641\n\230\204B\336\026Py\250\326\247gj\250\"\350\203\242\371\375:\303\003\370\273UT\303\t\234M>\370%\302\300\02
4&g\310u\355\207z\335Z`$\350\263\004\266D\210\2623@\271\226\224\023R\275\316\221+W][\244\006\034\272\335\275\353q8;\004\371\244\243T
\354\327\360L\210\221\265j-n\203L\364\350\376e,\342\323\332n\263\217G\004\350\232\230V\361\003`\245\337\244\201\2740\201\271\240\003
\002\001\027\242\201\261\004\201\256\032=\222\263s\fuj\264\345V[\272\355X\373\034\000P\246D@1\322\025\260\027\320#\346\300\aQ4\213\3
67\307\3056\233\a\243|\224V\363\355\205xM\207\241;\3168\261\211\371_\t\346\\\030o\373\277\036)s\203?\341k\v8\261\333\250?\211\354\27
0\030yvR\301\253\225\245\317\236\367\016C\214xE\321\314\373\003(2<E\304\020\003\004\3622\033~\254s[\311\311\214\212f\240\207\002^j\3
56\200\026G:\210\230\031\355\3165\314\223@\227\353\360P\311(\276\253^\021\365[\275\302\345\305\244]*\032ZD;\001\030\021(\232\330Q.H"
auths {
  method: "KERBEROS"
  mechanism: "GSSAPI"
  protocol: "hdfs"
  serverId: "lbdp014a.rnd.pncint.net"
}

DEBUG SaslRpcClient - Received SASL message state: CHALLENGE
token: "`f\006\t*\206H\206\367\022\001\002\002\002\000oW0U\240\003\002\001\005\241\003\002\001\017\242I0G\240\003\002\001\027\242@\0
04>\004\255$\230\355\354T\343k]\226\323\026\234\252\205,\031ru\364\232\001\242\232\260\210j\235\221\\\251{\2646\230!\222C\271-\356fj
80\035r\304\264\025<\205\214Y\314~\345\216[\204\360"

DEBUG SaslRpcClient - Sending sasl message state: RESPONSE
token: ""

DEBUG SaslRpcClient - Received SASL message state: CHALLENGE
token: "`0\006\t*\206H\206\367\022\001\002\002\002\001\021\000\377\377\377\377G\200\315\373q\030g\267\370\252\227\302\343\214\2432\2
33}.^G\227\265b\001\001\000\000\001"

DEBUG SaslRpcClient - Sending sasl message state: RESPONSE
token: "`0\006\t*\206H\206\367\022\001\002\002\002\001\021\000\377\377\377\377\354u\205\250\216\377C\201\247\250\316$\va\377/k\027\0
31\362\354\212\t\245\001\001\000\000\001"

DEBUG SaslRpcClient - Received SASL message state: SUCCESS

DEBUG Client - Negotiated QOP is :auth
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM: starting, hav
ing connections 2
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #27
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #27
DEBUG ProtobufRpcEngine - Call: getFileInfo took 61ms
DEBUG DFSClient - /user/spark/applicationHistory/application_1488308813765_0068.inprogress: masked={ masked: rw-r--r--, unmasked: rw
-rw-rw- }
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #28
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #28
DEBUG ProtobufRpcEngine - Call: create took 3ms
DEBUG DFSClient - computePacketChunkSize: src=/user/spark/applicationHistory/application_1488308813765_0068.inprogress, chunkSize=51
6, chunksPerPacket=127, packetSize=65532
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #29
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #29
DEBUG ProtobufRpcEngine - Call: setPermission took 3ms
INFO  EventLoggingListener - Logging events to hdfs://nameservice1/user/spark/applicationHistory/application_1488308813765_0068
INFO  YarnClientSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=0, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=0
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 232 lastFlushOffset 0
DEBUG DFSClient - Queued packet 0
DEBUG DFSClient - Allocating new block
DEBUG DFSClient - Waiting for ack for: 0
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #30
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #30
DEBUG ProtobufRpcEngine - Call: addBlock took 3ms
DEBUG DFSClient - pipeline = DatanodeInfoWithStorage[10.3.113.28:1004,DS-7b63f65a-ed96-4159-94da-7f483b1a2c3d,DISK]
DEBUG DFSClient - pipeline = DatanodeInfoWithStorage[10.3.113.29:1004,DS-b9d98861-f3b6-4ed7-b028-1f75de7adf19,DISK]
DEBUG DFSClient - pipeline = DatanodeInfoWithStorage[10.3.113.25:1004,DS-08d8089b-962a-4cbd-8dc4-6c524a15e01a,DISK]
DEBUG DFSClient - Connecting to datanode 10.3.113.28:1004
DEBUG DFSClient - Send buf size 124928
DEBUG SaslDataTransferClient - SASL client skipping handshake in secured configuration with privileged port for addr = /10.3.113.28,
 datanodeId = DatanodeInfoWithStorage[10.3.113.28:1004,DS-7b63f65a-ed96-4159-94da-7f483b1a2c3d,DISK]
DEBUG DFSClient - nodes [DatanodeInfoWithStorage[10.3.113.28:1004,DS-7b63f65a-ed96-4159-94da-7f483b1a2c3d,DISK], DatanodeInfoWithSto
rage[10.3.113.29:1004,DS-b9d98861-f3b6-4ed7-b028-1f75de7adf19,DISK], DatanodeInfoWithStorage[10.3.113.25:1004,DS-08d8089b-962a-4cbd-
8dc4-6c524a15e01a,DISK]] storageTypes [DISK, DISK, DISK] storageIDs [DS-7b63f65a-ed96-4159-94da-7f483b1a2c3d, DS-b9d98861-f3b6-4ed7-
b028-1f75de7adf19, DS-08d8089b-962a-4cbd-8dc4-6c524a15e01a]
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 0 o
ffsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 232
DEBUG DFSClient - DFSClient seqno: 0 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 1263712
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #31
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #31
DEBUG ProtobufRpcEngine - Call: fsync took 5ms
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=1, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=0
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 39960 lastFlushOffset 232
DEBUG DFSClient - Queued packet 1
DEBUG DFSClient - Waiting for ack for: 1
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 1 o
ffsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 39960
DEBUG DFSClient - DFSClient seqno: 1 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 1955934
DEBUG ClosureCleaner - +++ Cleaning closure <function2> (org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1$$anon
fun$11) +++
DEBUG ClosureCleaner -  + declared fields: 2
DEBUG ClosureCleaner -      public static final long org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1$$anonfun$
11.serialVersionUID
DEBUG ClosureCleaner -      private final org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1 org.apache.spark.str
eaming.dstream.DStream$$anonfun$saveAsTextFiles$1$$anonfun$11.$outer
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1$$ano
nfun$11.apply(java.lang.Object,java.lang.Object)
DEBUG ClosureCleaner -      public final void org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1$$anonfun$11.appl
y(org.apache.spark.rdd.RDD,org.apache.spark.streaming.Time)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 2
DEBUG ClosureCleaner -      org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1
DEBUG ClosureCleaner -      org.apache.spark.streaming.dstream.DStream
DEBUG ClosureCleaner -  + outer objects: 2
DEBUG ClosureCleaner -      <function0>
DEBUG ClosureCleaner -      org.apache.spark.streaming.kafka.KafkaInputDStream@533a6956
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG ClosureCleaner -      (class org.apache.spark.streaming.dstream.DStream,Set())
DEBUG ClosureCleaner -      (class org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1,Set(suffix$2, prefix$2))
DEBUG ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.streaming.dstream.DStream,o
rg.apache.spark.streaming.kafka.KafkaInputDStream@533a6956)
DEBUG ClosureCleaner -  + cloning the object <function0> of class org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFile
s$1
DEBUG ClosureCleaner -  + cleaning cloned closure <function0> recursively (org.apache.spark.streaming.dstream.DStream$$anonfun$saveA
sTextFiles$1)
DEBUG ClosureCleaner - +++ Cleaning closure <function0> (org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1) +++
DEBUG ClosureCleaner -  + declared fields: 4
DEBUG ClosureCleaner -      public static final long org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1.serialVer
sionUID
DEBUG ClosureCleaner -      private final org.apache.spark.streaming.dstream.DStream org.apache.spark.streaming.dstream.DStream$$ano
nfun$saveAsTextFiles$1.$outer
DEBUG ClosureCleaner -      public final java.lang.String org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1.pref
ix$2
DEBUG ClosureCleaner -      public final java.lang.String org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1.suff
ix$2
DEBUG ClosureCleaner -  + declared methods: 3
DEBUG ClosureCleaner -      public final void org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1.apply()
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1.appl
y()
DEBUG ClosureCleaner -      public void org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1.apply$mcV$sp()
DEBUG ClosureCleaner -  + inner classes: 1
DEBUG ClosureCleaner -      org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1$$anonfun$11
DEBUG ClosureCleaner -  + outer classes: 1
DEBUG ClosureCleaner -      org.apache.spark.streaming.dstream.DStream
DEBUG ClosureCleaner -  + outer objects: 1
DEBUG ClosureCleaner -      org.apache.spark.streaming.kafka.KafkaInputDStream@533a6956
DEBUG ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG ClosureCleaner -      (class org.apache.spark.streaming.dstream.DStream,Set())
DEBUG ClosureCleaner -      (class org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1,Set(suffix$2, prefix$2))
DEBUG ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.streaming.dstream.DStream,o
rg.apache.spark.streaming.kafka.KafkaInputDStream@533a6956)
DEBUG ClosureCleaner -  + the starting closure doesn't actually need org.apache.spark.streaming.kafka.KafkaInputDStream@533a6956, so
 we null it out
DEBUG ClosureCleaner -  +++ closure <function0> (org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1) is now clean
ed +++
DEBUG ClosureCleaner -  +++ closure <function2> (org.apache.spark.streaming.dstream.DStream$$anonfun$saveAsTextFiles$1$$anonfun$11)
is now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function1> (com.spark.stream.SparkKafkaLog$$anonfun$main$1) +++
DEBUG ClosureCleaner -  + declared fields: 1
DEBUG ClosureCleaner -      public static final long com.spark.stream.SparkKafkaLog$$anonfun$main$1.serialVersionUID
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public final java.lang.Object com.spark.stream.SparkKafkaLog$$anonfun$main$1.apply(java.lang.Object)
DEBUG ClosureCleaner -      public final java.lang.String com.spark.stream.SparkKafkaLog$$anonfun$main$1.apply(scala.Tuple2)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function1> (com.spark.stream.SparkKafkaLog$$anonfun$main$1) is now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function1> (com.spark.stream.SparkKafkaLog$$anonfun$main$2) +++
DEBUG ClosureCleaner -  + declared fields: 2
DEBUG ClosureCleaner -      public static final long com.spark.stream.SparkKafkaLog$$anonfun$main$2.serialVersionUID
DEBUG ClosureCleaner -      private final java.lang.String com.spark.stream.SparkKafkaLog$$anonfun$main$2.dataFormatted$1
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public final java.lang.Object com.spark.stream.SparkKafkaLog$$anonfun$main$2.apply(java.lang.Object)
DEBUG ClosureCleaner -      public final void com.spark.stream.SparkKafkaLog$$anonfun$main$2.apply(org.apache.spark.rdd.RDD)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function1> (com.spark.stream.SparkKafkaLog$$anonfun$main$2) is now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function2> (org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$a
pply$mcV$sp$3) +++
DEBUG ClosureCleaner -  + declared fields: 2
DEBUG ClosureCleaner -      public static final long org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply
$mcV$sp$3.serialVersionUID
DEBUG ClosureCleaner -      private final scala.Function1 org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$
apply$mcV$sp$3.cleanedF$1
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$
apply$mcV$sp$3.apply(java.lang.Object,java.lang.Object)
DEBUG ClosureCleaner -      public final void org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp
$3.apply(org.apache.spark.rdd.RDD,org.apache.spark.streaming.Time)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function2> (org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV
$sp$3) is now cleaned +++
Testing Kafka
WARN  StreamingContext - Dynamic Allocation is enabled for this application. Enabling Dynamic allocation for Spark Streaming applica
tions can cause data loss if Write Ahead Log is not enabled for non-replayable sources like Flume. See the programming guide for det
ails on how to enable the Write Ahead Log
DEBUG JobScheduler - Starting JobScheduler
DEBUG ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySpark
Job$2) +++
DEBUG ClosureCleaner -  + declared fields: 1
DEBUG ClosureCleaner -      public static final long org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$
2.serialVersionUID
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySpar
kJob$2.apply(java.lang.Object)
DEBUG ClosureCleaner -      public final scala.Tuple2 org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob
$2.apply(int)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function1> (org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$2) i
s now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$apply
$16) +++
DEBUG ClosureCleaner -  + declared fields: 1
DEBUG ClosureCleaner -      public static final long org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$apply$16.
serialVersionUID
DEBUG ClosureCleaner -  + declared methods: 1
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$appl
y$16.apply(java.lang.Object)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function1> (org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$1$$anonfun$apply$16) is
now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function2> (org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySpark
Job$1) +++
DEBUG ClosureCleaner -  + declared fields: 1
DEBUG ClosureCleaner -      public static final long org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$
1.serialVersionUID
DEBUG ClosureCleaner -  + declared methods: 3
DEBUG ClosureCleaner -      public final int org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$1.apply(
int,int)
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySpar
kJob$1.apply(java.lang.Object,java.lang.Object)
DEBUG ClosureCleaner -      public int org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$1.apply$mcIII$
sp(int,int)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function2> (org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$1) i
s now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function2> (org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySpark
Job$1) +++
DEBUG ClosureCleaner -  + declared fields: 1
DEBUG ClosureCleaner -      public static final long org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$
1.serialVersionUID
DEBUG ClosureCleaner -  + declared methods: 3
DEBUG ClosureCleaner -      public final int org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$1.apply(
int,int)
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySpar
kJob$1.apply(java.lang.Object,java.lang.Object)
DEBUG ClosureCleaner -      public int org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$1.apply$mcIII$
sp(int,int)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function2> (org.apache.spark.streaming.scheduler.ReceiverTracker$$anonfun$runDummySparkJob$1) i
s now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) +++
DEBUG ClosureCleaner -  + declared fields: 2
DEBUG ClosureCleaner -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.serialVersionUID
DEBUG ClosureCleaner -      private final org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$a
nonfun$12.$outer
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(java.lang.Ob
ject)
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(scala.collec
tion.Iterator)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 2
DEBUG ClosureCleaner -      org.apache.spark.rdd.RDD$$anonfun$collect$1
DEBUG ClosureCleaner -      org.apache.spark.rdd.RDD
DEBUG ClosureCleaner -  + outer objects: 2
DEBUG ClosureCleaner -      <function0>
DEBUG ClosureCleaner -      ShuffledRDD[2] at start at SparkKafkaLog.scala:34
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG ClosureCleaner -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
DEBUG ClosureCleaner -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
DEBUG ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.rdd.RDD,ShuffledRDD[2] at s
tart at SparkKafkaLog.scala:34)
DEBUG ClosureCleaner -  + cloning the object <function0> of class org.apache.spark.rdd.RDD$$anonfun$collect$1
DEBUG ClosureCleaner -  + cleaning cloned closure <function0> recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
DEBUG ClosureCleaner - +++ Cleaning closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
DEBUG ClosureCleaner -  + declared fields: 2
DEBUG ClosureCleaner -      public static final long org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
DEBUG ClosureCleaner -      private final org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$outer
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$an
onfun$$$outer()
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
DEBUG ClosureCleaner -  + inner classes: 1
DEBUG ClosureCleaner -      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12
DEBUG ClosureCleaner -  + outer classes: 1
DEBUG ClosureCleaner -      org.apache.spark.rdd.RDD
DEBUG ClosureCleaner -  + outer objects: 1
DEBUG ClosureCleaner -      ShuffledRDD[2] at start at SparkKafkaLog.scala:34
DEBUG ClosureCleaner -  + fields accessed by starting closure: 2
DEBUG ClosureCleaner -      (class org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
DEBUG ClosureCleaner -      (class org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($outer))
DEBUG ClosureCleaner -  + outermost object is not a closure, so do not clone it: (class org.apache.spark.rdd.RDD,ShuffledRDD[2] at s
tart at SparkKafkaLog.scala:34)
DEBUG ClosureCleaner -  +++ closure <function0> (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
DEBUG ClosureCleaner -  +++ closure <function1> (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12) is now cleaned +++
DEBUG ClosureCleaner - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
DEBUG ClosureCleaner -  + declared fields: 2
DEBUG ClosureCleaner -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
DEBUG ClosureCleaner -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
DEBUG ClosureCleaner -  + declared methods: 2
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,jav
a.lang.Object)
DEBUG ClosureCleaner -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.Tas
kContext,scala.collection.Iterator)
DEBUG ClosureCleaner -  + inner classes: 0
DEBUG ClosureCleaner -  + outer classes: 0
DEBUG ClosureCleaner -  + outer objects: 0
DEBUG ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG ClosureCleaner -  + there are no enclosing objects!
DEBUG ClosureCleaner -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
INFO  SparkContext - Starting job: start at SparkKafkaLog.scala:34
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #32
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #32
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
DEBUG SortShuffleManager - Can't use serialized shuffle for shuffle 0 because an aggregator is defined
INFO  DAGScheduler - Registering RDD 1 (start at SparkKafkaLog.scala:34)
INFO  DAGScheduler - Got job 0 (start at SparkKafkaLog.scala:34) with 20 output partitions
INFO  DAGScheduler - Final stage: ResultStage 1 (start at SparkKafkaLog.scala:34)
INFO  DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
INFO  DAGScheduler - Missing parents: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DAGScheduler - missing: List()
INFO  DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[1] at start at SparkKafkaLog.scala:34), which has no missing par
ents
DEBUG DAGScheduler - submitMissingTasks(ShuffleMapStage 0)
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=2, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=39936
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 44313 lastFlushOffset 39960
DEBUG DFSClient - Queued packet 2
DEBUG DFSClient - Waiting for ack for: 2
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 2 o
ffsetInBlock: 39936 lastPacketInBlock: false lastByteOffsetInBlock: 44313
DEBUG DFSClient - DFSClient seqno: 2 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 3168373
DEBUG ExecutorAllocationManager - Starting timer to add executors because pending tasks are building up (to expire in 1 seconds)
INFO  MemoryStore - Block broadcast_0 stored as values in memory (estimated size 2.8 KB, free 530.3 MB)
DEBUG BlockManager - Put block broadcast_0 locally took  60 ms
DEBUG BlockManager - Putting block broadcast_0 without replication took  61 ms
INFO  MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1709.0 B, free 530.3 MB)
INFO  BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.3.112.22:36031 (size: 1709.0 B, free: 530.3 MB)
DEBUG BlockManagerMaster - Updated info of block broadcast_0_piece0
DEBUG BlockManager - Told master about block broadcast_0_piece0
DEBUG BlockManager - Put block broadcast_0_piece0 locally took  4 ms
DEBUG BlockManager - Putting block broadcast_0_piece0 without replication took  4 ms
INFO  SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
INFO  DAGScheduler - Submitting 50 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[1] at start at SparkKafkaLog.scala:34)
DEBUG DAGScheduler - New pending partitions: Set(0, 30, 9, 1, 31, 2, 32, 24, 3, 25, 4, 47, 26, 48, 27, 19, 49, 20, 42, 21, 43, 22, 1
4, 44, 23, 15, 45, 37, 16, 46, 38, 17, 39, 18, 10, 40, 11, 41, 33, 12, 34, 13, 5, 35, 6, 36, 28, 7, 29, 8)
INFO  YarnScheduler - Adding task set 0.0 with 50 tasks
DEBUG TaskSetManager - Epoch for TaskSet 0.0: 0
DEBUG TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #33
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #33
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  ExecutorAllocationManager - Requesting 1 new executor because tasks are backlogged (new desired total will be 1)
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #34
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #34
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  ExecutorAllocationManager - Requesting 2 new executors because tasks are backlogged (new desired total will be 3)
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #35
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #35
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 2ms
INFO  ExecutorAllocationManager - Requesting 4 new executors because tasks are backlogged (new desired total will be 7)
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #36
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #36
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 2ms
INFO  ExecutorAllocationManager - Requesting 8 new executors because tasks are backlogged (new desired total will be 15)
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #37
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #37
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  ExecutorAllocationManager - Requesting 16 new executors because tasks are backlogged (new desired total will be 31)
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #38
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #38
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
INFO  ExecutorAllocationManager - Requesting 19 new executors because tasks are backlogged (new desired total will be 50)
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #39
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #39
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 1ms
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
INFO  YarnClientSchedulerBackend - Registered executor NettyRpcEndpointRef(null) (lbdp008a.rnd.pncint.net:44672) with ID 10
DEBUG YarnClientSchedulerBackend - Decremented number of pending executors (49 left)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=3, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=44032
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 47647 lastFlushOffset 44313
DEBUG DFSClient - Queued packet 3
DEBUG DFSClient - Waiting for ack for: 3
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 3 o
ffsetInBlock: 44032 lastPacketInBlock: false lastByteOffsetInBlock: 47647
DEBUG DFSClient - DFSClient seqno: 3 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 2849555
DEBUG ExecutorAllocationManager - Starting idle timer for 10 because there are no more tasks scheduled to run on the executor (to ex
pire in 60 seconds)
INFO  ExecutorAllocationManager - New executor 10 has registered (new total is 1)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
INFO  TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, lbdp008a.rnd.pncint.net, executor 10, partition 0, PROCESS_LOCAL, 2015
 bytes)
INFO  BlockManagerMasterEndpoint - Registering block manager lbdp008a.rnd.pncint.net:44994 with 530.3 MB RAM, BlockManagerId(10, lbd
p008a.rnd.pncint.net, 44994)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=4, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=47616
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 47829 lastFlushOffset 47647
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DFSClient - Queued packet 4
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DFSClient - Waiting for ack for: 4
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 4 o
ffsetInBlock: 47616 lastPacketInBlock: false lastByteOffsetInBlock: 47829
DEBUG DFSClient - DFSClient seqno: 4 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 4165330
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
INFO  YarnClientSchedulerBackend - Registered executor NettyRpcEndpointRef(null) (lbdp010a.rnd.pncint.net:35828) with ID 3
DEBUG YarnClientSchedulerBackend - Decremented number of pending executors (48 left)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=5, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=47616
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 48581 lastFlushOffset 47829
DEBUG DFSClient - Queued packet 5
DEBUG DFSClient - Waiting for ack for: 5
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 5 o
ffsetInBlock: 47616 lastPacketInBlock: false lastByteOffsetInBlock: 48581
DEBUG DFSClient - DFSClient seqno: 5 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 13713164
DEBUG ExecutorAllocationManager - Starting idle timer for 3 because there are no more tasks scheduled to run on the executor (to exp
ire in 60 seconds)
INFO  ExecutorAllocationManager - New executor 3 has registered (new total is 2)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
INFO  TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, lbdp010a.rnd.pncint.net, executor 3, partition 1, PROCESS_LOCAL, 2015
bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 3 because it is now running a task
INFO  BlockManagerMasterEndpoint - Registering block manager lbdp010a.rnd.pncint.net:33561 with 530.3 MB RAM, BlockManagerId(3, lbdp
010a.rnd.pncint.net, 33561)
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=6, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=48128
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 49078 lastFlushOffset 48581
DEBUG DFSClient - Queued packet 6
DEBUG DFSClient - Waiting for ack for: 6
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 6 o
ffsetInBlock: 48128 lastPacketInBlock: false lastByteOffsetInBlock: 49078
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 2
DEBUG DFSClient - DFSClient seqno: 6 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 38060451
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #40
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #40
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 2ms
DEBUG BlockManager - Level for block broadcast_0_piece0 is StorageLevel(true, true, false, false, 1)
DEBUG BlockManager - Getting block broadcast_0_piece0 from memory
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
INFO  BlockManagerInfo - Added broadcast_0_piece0 in memory on lbdp008a.rnd.pncint.net:44994 (size: 1709.0 B, free: 530.3 MB)
DEBUG BlockManager - Level for block broadcast_0_piece0 is StorageLevel(true, true, false, false, 1)
DEBUG BlockManager - Getting block broadcast_0_piece0 from memory
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  BlockManagerInfo - Added broadcast_0_piece0 in memory on lbdp010a.rnd.pncint.net:33561 (size: 1709.0 B, free: 530.3 MB)
INFO  TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, lbdp008a.rnd.pncint.net, executor 10, partition 2, PROCESS_LOCAL, 2015
 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
INFO  TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 732 ms on lbdp008a.rnd.pncint.net (executor 10) (1/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, lbdp008a.rnd.pncint.net, executor 10, partition 3, PROCESS_LOCAL, 2015
 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
INFO  TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 58 ms on lbdp008a.rnd.pncint.net (executor 10) (2/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, lbdp008a.rnd.pncint.net, executor 10, partition 4, PROCESS_LOCAL, 2015
 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
DEBUG ExecutorAllocationManager - Lowering target number of executors to 48 (previously 50) because not all requested executors are
actually needed
INFO  TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 57 ms on lbdp008a.rnd.pncint.net (executor 10) (3/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, lbdp008a.rnd.pncint.net, executor 10, partition 5, PROCESS_LOCAL, 2015
 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
INFO  TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 55 ms on lbdp008a.rnd.pncint.net (executor 10) (4/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, lbdp010a.rnd.pncint.net, executor 3, partition 6, PROCESS_LOCAL, 2015
bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 3 because it is now running a task
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, lbdp008a.rnd.pncint.net, executor 10, partition 7, PROCESS_LOCAL, 2015
 bytes)
INFO  TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 681 ms on lbdp010a.rnd.pncint.net (executor 3) (5/50)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DAGScheduler - ShuffleMapTask finished on 3
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
INFO  TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 59 ms on lbdp008a.rnd.pncint.net (executor 10) (6/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG ExecutorAllocationManager - Lowering target number of executors to 44 (previously 48) because not all requested executors are
actually needed
INFO  TaskSetManager - Starting task 8.0 in stage 0.0 (TID 8, lbdp010a.rnd.pncint.net, executor 3, partition 8, PROCESS_LOCAL, 2015
bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 3 because it is now running a task
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 9.0 in stage 0.0 (TID 9, lbdp008a.rnd.pncint.net, executor 10, partition 9, PROCESS_LOCAL, 2015
 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
INFO  TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 63 ms on lbdp010a.rnd.pncint.net (executor 3) (7/50)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DAGScheduler - ShuffleMapTask finished on 3
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
^CINFO  TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 61 ms on lbdp008a.rnd.pncint.net (executor 10) (8/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
INFO  SparkContext - Invoking stop() from shutdown hook
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.Server@67d207b2
DEBUG AbstractLifeCycle - stopping SelectChannelConnector@0.0.0.0:4040
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=7, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=48640
DEBUG DFSClient - DFSClient flush():  bytesCurBlock 58145 lastFlushOffset 49078
DEBUG DFSClient - Queued packet 7
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@29d2e740
DEBUG DFSClient - Waiting for ack for: 7
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 7 o
ffsetInBlock: 48640 lastPacketInBlock: false lastByteOffsetInBlock: 58145
DEBUG nio - Stopped Thread[qtp527641588-64 Selector0,5,main] on org.spark-project.jetty.io.nio.SelectorManager$1@33784d10
DEBUG nio - Stopped Thread[qtp527641588-65 Selector1,5,main] on org.spark-project.jetty.io.nio.SelectorManager$1@1f0b6fd6
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #41
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #41
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG ProtobufRpcEngine - Call: renewLease took 1ms
DEBUG LeaseRenewer - Lease renewed for client DFSClient_NONMAPREDUCE_-279890539_1
DEBUG LeaseRenewer - Lease renewer daemon for [DFSClient_NONMAPREDUCE_-279890539_1] with renew id 1 executed
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@29d2e740
DEBUG AbstractLifeCycle - stopping PooledBuffers [0/1024@6144,0/1024@16384,0/1024@-]/PooledBuffers [0/1024@6144,0/1024@32768,0/1024@
-]
DEBUG AbstractLifeCycle - STOPPED null/null
DEBUG AbstractLifeCycle - STOPPED SelectChannelConnector@0.0.0.0:4040
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.ContextHandlerCollection@34849e52
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.ContextHandlerCollection@34849e52
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@6c14d50c
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@6c14d50c
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-35c91146
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-35c91146
INFO  TaskSetManager - Starting task 10.0 in stage 0.0 (TID 10, lbdp010a.rnd.pncint.net, executor 3, partition 10, PROCESS_LOCAL, 20
15 bytes)
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@6c14d50c
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG DAGScheduler - submitStage(ResultStage 1)
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/metrics/json,null} - org.spark-project.jetty.servlet.ServletHandler@6c14d
50c as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@6837fff2
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@4543f37
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@4543f37
DEBUG AbstractLifeCycle - stopping org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f
DEBUG AbstractLifeCycle - STOPPED org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter-6b81b95f
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$3-7259f1b0
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$3-7259f1b0
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@4543f37
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/stages/stage/kill,null} - org.spark-project.jetty.servlet.ServletHandler@
4543f37 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@6837fff2
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@6837fff2
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@6d688223
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/api,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@1e4fba5d
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@1e4fba5d
DEBUG AbstractLifeCycle - stopping com.sun.jersey.spi.container.servlet.ServletContainer-6b1bb196
DEBUG AbstractLifeCycle - STOPPED com.sun.jersey.spi.container.servlet.ServletContainer-6b1bb196
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@1e4fba5d
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/api,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/api,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/api,null} - org.spark-project.jetty.servlet.ServletHandler@1e4fba5d as ha
ndler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/api,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@6d688223
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@6d688223
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@5da4c884
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@6c3355f2
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@6c3355f2
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$3-5c35d5da
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$3-5c35d5da
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@6c3355f2
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/,null} - org.spark-project.jetty.servlet.ServletHandler@6c3355f2 as handl
er
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@5da4c884
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@5da4c884
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@30215a23
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/static,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@7d9ae787
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@7d9ae787
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.DefaultServlet-17b45674
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.DefaultServlet-17b45674
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@7d9ae787
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/static,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/static,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/static,null} - org.spark-project.jetty.servlet.ServletHandler@7d9ae787 as
 handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/static,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@30215a23
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@30215a23
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@31a69774
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@50d1cbcc
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@50d1cbcc
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-5c413eb8
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-5c413eb8
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@50d1cbcc
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/executors/threadDump/json,null} - org.spark-project.jetty.servlet.Servlet
Handler@50d1cbcc as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@31a69774
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@31a69774
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@c840dcf
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/executors/threadDump,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@2e574e38
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@2e574e38
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-7cc8b288
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-7cc8b288
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@2e574e38
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/executors/threadDump,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/executors/threadDump,null} - org.spark-project.jetty.servlet.ServletHandl
er@2e574e38 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/executors/threadDump,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@c840dcf
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@c840dcf
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@6e4752a9
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/executors/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@444b5767
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@444b5767
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-15436088
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-15436088
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@444b5767
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/executors/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors/json,null}
INFO  TaskSetManager - Starting task 11.0 in stage 0.0 (TID 11, lbdp008a.rnd.pncint.net, executor 10, partition 11, PROCESS_LOCAL, 2
015 bytes)
DEBUG Container - Container o.s.j.s.ServletContextHandler{/executors/json,null} - org.spark-project.jetty.servlet.ServletHandler@444
b5767 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/executors/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@6e4752a9
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@6e4752a9
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@70a7284e
INFO  TaskSetManager - Finished task 8.0 in stage 0.0 (TID 8) in 59 ms on lbdp010a.rnd.pncint.net (executor 3) (9/50)
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/executors,null}
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@31d1ca96
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@31d1ca96
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-3c97d96c
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-3c97d96c
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@31d1ca96
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/executors,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/executors,null}
DEBUG DAGScheduler - ShuffleMapTask finished on 3
DEBUG Container - Container o.s.j.s.ServletContextHandler{/executors,null} - org.spark-project.jetty.servlet.ServletHandler@31d1ca96
 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/executors,null}
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@70a7284e
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@70a7284e
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@11302852
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/environment/json,null}
DEBUG DFSClient - DFSClient seqno: 7 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 30760619
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@7f39e4a3
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@7f39e4a3
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-2f1f3c8b
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2f1f3c8b
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@7f39e4a3
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/environment/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/environment/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/environment/json,null} - org.spark-project.jetty.servlet.ServletHandler@7
f39e4a3 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/environment/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@11302852
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@11302852
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@4ac22976
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/environment,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@b937a2c
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@b937a2c
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-7b0f62cb
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-7b0f62cb
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@b937a2c
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/environment,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/environment,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/environment,null} - org.spark-project.jetty.servlet.ServletHandler@b937a2
c as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/environment,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@4ac22976
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@4ac22976
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@598d4938
DEBUG ExecutorAllocationManager - Clearing idle timer for 3 because it is now running a task
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@772c8414
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@772c8414
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-3abfa5d0
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-3abfa5d0
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@772c8414
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/storage/rdd/json,null} - org.spark-project.jetty.servlet.ServletHandler@7
72c8414 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@598d4938
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@598d4938
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@e931229
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/storage/rdd,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@4f806802
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@4f806802
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-12efc256
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-12efc256
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@4f806802
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/storage/rdd,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/storage/rdd,null} - org.spark-project.jetty.servlet.ServletHandler@4f8068
02 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/storage/rdd,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@e931229
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@e931229
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@4f0015bc
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/storage/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@3e5928b8
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@3e5928b8
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-2dfb9cf1
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2dfb9cf1
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@3e5928b8
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/storage/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/storage/json,null} - org.spark-project.jetty.servlet.ServletHandler@3e592
8b8 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/storage/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@4f0015bc
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@4f0015bc
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@620bba8
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/storage,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@4c1bf0b9
DEBUG ExecutorAllocationManager - Lowering target number of executors to 40 (previously 44) because not all requested executors are
actually needed
DEBUG DAGScheduler - ShuffleMapTask finished on 10
INFO  TaskSetManager - Finished task 9.0 in stage 0.0 (TID 9) in 54 ms on lbdp008a.rnd.pncint.net (executor 10) (10/50)
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@4c1bf0b9
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-5942c2af
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-5942c2af
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@4c1bf0b9
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/storage,null}
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/storage,null}
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG Container - Container o.s.j.s.ServletContextHandler{/storage,null} - org.spark-project.jetty.servlet.ServletHandler@4c1bf0b9 a
s handler
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/storage,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@620bba8
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@620bba8
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@1d61e57a
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages/pool/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@74bf5798
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@74bf5798
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-2c2968d6
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2c2968d6
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@74bf5798
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages/pool/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/stages/pool/json,null} - org.spark-project.jetty.servlet.ServletHandler@7
4bf5798 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages/pool/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@1d61e57a
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@1d61e57a
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@e117a05
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages/pool,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@29817f19
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@29817f19
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-74617c53
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-74617c53
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@29817f19
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages/pool,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/stages/pool,null} - org.spark-project.jetty.servlet.ServletHandler@29817f
19 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages/pool,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@e117a05
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@e117a05
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@7ffc50a0
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages/stage/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@3cc8e634
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@3cc8e634
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-2e05f47e
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2e05f47e
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@3cc8e634
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages/stage/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/stages/stage/json,null} - org.spark-project.jetty.servlet.ServletHandler@
3cc8e634 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages/stage/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@7ffc50a0
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@7ffc50a0
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@4f22f4e6
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages/stage,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@4bc814ba
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@4bc814ba
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-2e69f3d0
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2e69f3d0
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@4bc814ba
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages/stage,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/stages/stage,null} - org.spark-project.jetty.servlet.ServletHandler@4bc81
4ba as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages/stage,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@4f22f4e6
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@4f22f4e6
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@144d0f3b
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@1d036c7f
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@1d036c7f
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-1b393f8f
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1b393f8f
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@1d036c7f
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/stages/json,null} - org.spark-project.jetty.servlet.ServletHandler@1d036c
7f as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@144d0f3b
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@144d0f3b
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@73003ba9
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@55e4c027
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@55e4c027
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-6f48e70d
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-6f48e70d
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@55e4c027
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages,null}
DEBUG ExecutorAllocationManager - Starting timer to add more executors (to expire in 1 seconds)
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages,null}
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM sending #42
DEBUG Container - Container o.s.j.s.ServletContextHandler{/stages,null} - org.spark-project.jetty.servlet.ServletHandler@55e4c027 as
 handler
INFO  TaskSetManager - Starting task 12.0 in stage 0.0 (TID 12, lbdp010a.rnd.pncint.net, executor 3, partition 12, PROCESS_LOCAL, 20
15 bytes)
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@73003ba9
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@73003ba9
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@5e832150
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/jobs/job/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@46660d14
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@46660d14
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-6f89cd7e
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-6f89cd7e
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@46660d14
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/jobs/job/json,null}
DEBUG ExecutorAllocationManager - Clearing idle timer for 3 because it is now running a task
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/jobs/job/json,null} - org.spark-project.jetty.servlet.ServletHandler@4666
0d14 as handler
DEBUG Client - IPC Client (723814746) connection to lbdp005a.rnd.pncint.net/10.3.112.24:8032 from PL24116@PNCBANK.COM got value #42
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/jobs/job/json,null}
DEBUG ProtobufRpcEngine - Call: getApplicationReport took 274ms
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@5e832150
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@5e832150
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@36daeb22
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/jobs/job,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@35b80d21
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@35b80d21
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-4c12f0f6
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-4c12f0f6
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@35b80d21
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/jobs/job,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/jobs/job,null} - org.spark-project.jetty.servlet.ServletHandler@35b80d21
as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/jobs/job,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@36daeb22
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@36daeb22
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@3fea6a16
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/jobs/json,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@4099dde9
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@4099dde9
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-df9d3bc
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-df9d3bc
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@4099dde9
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/jobs/json,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/jobs/json,null} - org.spark-project.jetty.servlet.ServletHandler@4099dde9
 as handler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/jobs/json,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@3fea6a16
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@3fea6a16
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@62eb3516
DEBUG AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/jobs,null}
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@52f1234c
DEBUG AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@52f1234c
DEBUG AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-4ee8ff06
DEBUG AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-4ee8ff06
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@52f1234c
DEBUG AbstractHandler - stopping o.s.j.s.ServletContextHandler{/jobs,null}
INFO  ContextHandler - stopped o.s.j.s.ServletContextHandler{/jobs,null}
DEBUG Container - Container o.s.j.s.ServletContextHandler{/jobs,null} - org.spark-project.jetty.servlet.ServletHandler@52f1234c as h
andler
DEBUG AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/jobs,null}
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@62eb3516
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@62eb3516
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.ContextHandlerCollection@34849e52
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.Server@67d207b2
DEBUG AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.ErrorHandler@3564b98c
DEBUG AbstractHandler - stopping org.spark-project.jetty.server.handler.ErrorHandler@3564b98c
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.ErrorHandler@3564b98c
DEBUG AbstractLifeCycle - stopping qtp527641588{8<=8<=8/254,0}
INFO  TaskSetManager - Starting task 13.0 in stage 0.0 (TID 13, lbdp008a.rnd.pncint.net, executor 10, partition 13, PROCESS_LOCAL, 2
015 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
INFO  TaskSetManager - Finished task 10.0 in stage 0.0 (TID 10) in 733 ms on lbdp010a.rnd.pncint.net (executor 3) (11/50)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DAGScheduler - ShuffleMapTask finished on 3
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 2
INFO  TaskSetManager - Finished task 11.0 in stage 0.0 (TID 11) in 731 ms on lbdp008a.rnd.pncint.net (executor 10) (12/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 14.0 in stage 0.0 (TID 14, lbdp010a.rnd.pncint.net, executor 3, partition 14, PROCESS_LOCAL, 20
15 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 3 because it is now running a task
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 15.0 in stage 0.0 (TID 15, lbdp008a.rnd.pncint.net, executor 10, partition 15, PROCESS_LOCAL, 2
015 bytes)
DEBUG DAGScheduler - submitStage(ResultStage 1)
INFO  TaskSetManager - Finished task 12.0 in stage 0.0 (TID 12) in 531 ms on lbdp010a.rnd.pncint.net (executor 3) (13/50)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DAGScheduler - ShuffleMapTask finished on 3
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG ExecutorAllocationManager - Clearing idle timer for 10 because it is now running a task
DEBUG AbstractLifeCycle - STOPPED qtp527641588{8<=0<=0/254,6}
DEBUG AbstractLifeCycle - STOPPED org.spark-project.jetty.server.Server@67d207b2
INFO  SparkUI - Stopped Spark web UI at http://10.3.112.22:4040
INFO  TaskSetManager - Finished task 13.0 in stage 0.0 (TID 13) in 59 ms on lbdp008a.rnd.pncint.net (executor 10) (14/50)
DEBUG DAGScheduler - ShuffleMapTask finished on 10
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG DFSClient - DFSClient writeChunk allocating new packet seqno=8, src=/user/spark/applicationHistory/application_1488308813765_0
068.inprogress, packetSize=65532, chunksPerPacket=127, bytesCurBlock=57856
DEBUG DFSClient - Queued packet 8
DEBUG DFSClient - Queued packet 9
DEBUG DFSClient - Waiting for ack for: 9
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 8 o
ffsetInBlock: 57856 lastPacketInBlock: false lastByteOffsetInBlock: 64913
DEBUG DFSClient - DFSClient seqno: 8 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 4544930
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG DFSClient - DataStreamer block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452 sending packet packet seqno: 9 o
ffsetInBlock: 64913 lastPacketInBlock: true lastByteOffsetInBlock: 64913
DEBUG DFSClient - DFSClient seqno: 9 reply: 0 reply: 0 reply: 0 downstreamAckTimeNanos: 5894531
DEBUG DFSClient - Closing old block BP-1978894624-10.3.112.24-1435864746178:blk_1082229042_8583452
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #43
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #43
DEBUG ProtobufRpcEngine - Call: complete took 4ms
INFO  TaskSetManager - Starting task 16.0 in stage 0.0 (TID 16, lbdp010a.rnd.pncint.net, executor 3, partition 16, PROCESS_LOCAL, 20
15 bytes)
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #44
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #44
DEBUG ProtobufRpcEngine - Call: getFileInfo took 1ms
ERROR LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerTaskStart(0,0,org.apache.spark.scheduler.T
askInfo@98b84ac)
DEBUG DAGScheduler - submitStage(ResultStage 1)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
DEBUG DAGScheduler - missing: List(ShuffleMapStage 0)
DEBUG DAGScheduler - submitStage(ShuffleMapStage 0)
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #45
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #45
DEBUG ProtobufRpcEngine - Call: rename took 3ms
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM sending #46
DEBUG Client - IPC Client (723814746) connection to lbdp014a.rnd.pncint.net/10.3.112.25:8020 from PL24116@PNCBANK.COM got value #46
DEBUG ProtobufRpcEngine - Call: setTimes took 0ms
INFO  TaskSetManager - Starting task 17.0 in stage 0.0 (TID 17, lbdp008a.rnd.pncint.net, executor 10, partition 17, PROCESS_LOCAL, 2
015 bytes)
INFO  DAGScheduler - Job 0 failed: start at SparkKafkaLog.scala:34, took 9.592967 s
INFO  DAGScheduler - ShuffleMapStage 0 (start at SparkKafkaLog.scala:34) failed in 9.169 s due to Stage cancelled because SparkConte
xt was shut down
INFO  TaskSetManager - Finished task 14.0 in stage 0.0 (TID 14) in 152 ms on lbdp010a.rnd.pncint.net (executor 3) (15/50)
ERROR LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.
StageInfo@295e9e2b)
ERROR StreamingContext - Error starting the context, marking it as stopped
org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)
        at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
        at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1662)
        at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
        at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1585)
        at org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1748)
        at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1220)
        at org.apache.spark.SparkContext.stop(SparkContext.scala:1747)
        at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:604)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager
.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:
239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:
239)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1818)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1840)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1853)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1866)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1937)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
        at org.apache.spark.streaming.scheduler.ReceiverTracker.runDummySparkJob(ReceiverTracker.scala:404)
        at org.apache.spark.streaming.scheduler.ReceiverTracker.launchReceivers(ReceiverTracker.scala:420)
        at org.apache.spark.streaming.scheduler.ReceiverTracker.start(ReceiverTracker.scala:157)
        at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:82)
        at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:610)
        at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:606)
        at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:606)
        at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
        at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:606)
        at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:600)
        at com.spark.stream.SparkKafkaLog$.main(SparkKafkaLog.scala:34)
        at com.spark.stream.SparkKafkaLog.main(SparkKafkaLog.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
ERROR LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1489695321770,JobFailed(org.apach
e.spark.SparkException: Job 0 cancelled because SparkContext was shut down))
DEBUG JobScheduler - Stopping JobScheduler
DEBUG JobScheduler - Stopping job executor
DEBUG JobScheduler - Stopped job executor
INFO  JobScheduler - Stopped JobScheduler
Exception in thread "main" org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)
INFO  YarnClientSchedulerBackend - Interrupting monitor thread
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)
        at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)INFO  YarnClientSchedulerBackend - Shutting down all executors

        at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1662)
        at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)INFO  TaskSetManager - Finished task 15.0 in stage 0.0 (TID 15) i
n 153 ms on lbdp008a.rnd.pncint.net (executor 10) (16/50)

        at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1585)
        at org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1748)
        at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1220)
        at org.apache.spark.SparkContext.stop(SparkContext.scala:1747)
        at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:604)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager
.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:
239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:
239)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1818)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1840)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1853)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1866)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1937)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
        at org.apache.spark.streaming.scheduler.ReceiverTracker.runDummySparkJob(ReceiverTracker.scala:404)
        at org.apache.spark.streaming.scheduler.ReceiverTracker.launchReceivers(ReceiverTracker.scala:420)
        at org.apache.spark.streaming.scheduler.ReceiverTracker.start(ReceiverTracker.scala:157)
        at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:82)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
        at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:610)
        at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:606)
        at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:606)
        at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
        at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:606)
        at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:600)
        at com.spark.stream.SparkKafkaLog$.main(SparkKafkaLog.scala:34)
        at com.spark.stream.SparkKafkaLog.main(SparkKafkaLog.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
INFO  TaskSetManager - Starting task 18.0 in stage 0.0 (TID 18, lbdp010a.rnd.pncint.net, executor 3, partition 18, PROCESS_LOCAL, 20
15 bytes)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 1
INFO  TaskSetManager - Starting task 19.0 in stage 0.0 (TID 19, lbdp008a.rnd.pncint.net, executor 10, partition 19, PROCESS_LOCAL, 2
015 bytes)
INFO  TaskSetManager - Finished task 16.0 in stage 0.0 (TID 16) in 279 ms on lbdp010a.rnd.pncint.net (executor 3) (17/50)
INFO  YarnClientSchedulerBackend - Registered executor NettyRpcEndpointRef(null) (lbdp010a.rnd.pncint.net:35858) with ID 23
INFO  TaskSetManager - Finished task 17.0 in stage 0.0 (TID 17) in 268 ms on lbdp008a.rnd.pncint.net (executor 10) (18/50)
ERROR LiveListenerBus - SparkListenerBus has already stopped! Dropping event SparkListenerExecutorAdded(1489695322018,23,org.apache.
spark.scheduler.cluster.ExecutorData@e8bb21f5)
DEBUG YarnScheduler - parentName: , name: TaskSet_0, runningTasks: 2
DEBUG TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
INFO  TaskSetManager - Starting task 20.0 in stage 0.0 (TID 20, lbdp010a.rnd.pncint.net, executor 23, partition 20, PROCESS_LOCAL, 2
015 bytes)
INFO  YarnClientSchedulerBackend - Asking each executor to shut down
DEBUG AbstractService - Service: org.apache.hadoop.yarn.client.api.impl.YarnClientImpl entered state STOPPED
DEBUG Client - stopping client from cache: org.apache.hadoop.ipc.Client@596df59
INFO  YarnClientSchedulerBackend - Stopped
INFO  MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
INFO  MemoryStore - MemoryStore cleared
INFO  BlockManager - BlockManager stopped
INFO  BlockManagerMaster - BlockManagerMaster stopped
INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
INFO  SparkContext - Successfully stopped SparkContext
INFO  ShutdownHookManager - Shutdown hook called
INFO  RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
INFO  ShutdownHookManager - Deleting directory /tmp/spark-7a04ff43-c1c3-462f-b9b1-c67318b6ab43
DEBUG Client - stopping client from cache: org.apache.hadoop.ipc.Client@596df59
[pl24116@VM:lbdp003a SparkStreaming]$ ^C
[pl24116@VM:lbdp003a SparkStreaming]$ timed out waiting for input: auto-logout
